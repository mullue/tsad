{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection for clickstream data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection with DeepAR Timeseries forecasting\n",
    "\n",
    "We will use variables and dataframes that we stored at 1.EDA step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r urls css streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,25))\n",
    "data = []\n",
    "for index, url in zip(range(len(urls)), urls):\n",
    "    r = css[css['url'] == url].set_index('timestamp').resample('10T')\n",
    "    l = {'start' : str(r.nunique().index[0]),\n",
    "         'target': list(r.sum()['clickstream_id'].values.astype('float')),\n",
    "         'dynamic_feat': [list(r.nunique()['user_session_id'].values.astype('float'))]\n",
    "        }\n",
    "    data.append(l)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))\n",
    "            \n",
    "write_dicts_to_file(\"train.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'deepar-clickstream'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-308961792850/deepar-clickstream/train.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s3 = sagemaker_session.upload_data(path='train.json', key_prefix=s3_prefix)\n",
    "train_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2012-03-01 00:00:00\", \"target\": [24.0, 22.0, 20.0, 17.0, 15.0, 12.0, 15.0, 10.0, 14.0, 9....\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(train_s3, 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker DeepAR Tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# we use 2 hour frequency for the time series\n",
    "freq = datetime.timedelta(minutes=10)\n",
    "\n",
    "# we predict for 24 hours\n",
    "prediction_length = 24 * 6\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 24 * 6\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-clickstream'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": '10min',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-11 16:23:31 Starting - Starting the training job...\n",
      "2020-08-11 16:23:34 Starting - Launching requested ML instances.........\n",
      "2020-08-11 16:25:17 Starting - Preparing the instances for training......\n",
      "2020-08-11 16:26:26 Downloading - Downloading input data\n",
      "2020-08-11 16:26:26 Training - Downloading the training image...\n",
      "2020-08-11 16:26:48 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'144', u'epochs': u'400', u'time_freq': u'10min', u'context_length': u'144', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'144', u'time_freq': u'10min', u'context_length': u'144', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=1 from dataset.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Training set statistics:\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Integer time series\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] number of time series: 16\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] number of observations: 34556\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] mean target length: 2159\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] min/mean/max target: 0.0/12.1908206968/477.0\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] mean abs(target): 12.1908206968\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] contains missing values: no\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Small number of time series. Doing 40 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] nvidia-smi took: 0.0252349376678 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:51 INFO 139712694728512] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 2290.36808013916, \"sum\": 2290.36808013916, \"min\": 2290.36808013916}}, \"EndTime\": 1597163214.021548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163211.73025}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:54 INFO 139712694728512] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 3818.9961910247803, \"sum\": 3818.9961910247803, \"min\": 3818.9961910247803}}, \"EndTime\": 1597163215.549391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163214.021639}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:56 INFO 139712694728512] Epoch[0] Batch[0] avg_epoch_loss=4.127233\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.12723302841\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:58 INFO 139712694728512] Epoch[0] Batch[5] avg_epoch_loss=3.689773\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.68977308273\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:26:58 INFO 139712694728512] Epoch[0] Batch [5]#011Speed: 183.93 samples/sec#011loss=3.689773\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:00 INFO 139712694728512] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 4455.446004867554, \"sum\": 4455.446004867554, \"min\": 4455.446004867554}}, \"EndTime\": 1597163220.005088, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163215.549511}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:00 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=138.028643639 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:00 INFO 139712694728512] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.58199129105\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:00 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:00 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_2d681414-d44d-4736-900a-e40a032d7035-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 179.31008338928223, \"sum\": 179.31008338928223, \"min\": 179.31008338928223}}, \"EndTime\": 1597163220.185243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163220.005192}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:01 INFO 139712694728512] Epoch[1] Batch[0] avg_epoch_loss=3.360096\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.36009573936\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:02 INFO 139712694728512] Epoch[1] Batch[5] avg_epoch_loss=3.310484\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.3104839325\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:02 INFO 139712694728512] Epoch[1] Batch [5]#011Speed: 180.69 samples/sec#011loss=3.310484\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:04 INFO 139712694728512] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4044.545888900757, \"sum\": 4044.545888900757, \"min\": 4044.545888900757}}, \"EndTime\": 1597163224.229953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163220.185334}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:04 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.749154018 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:04 INFO 139712694728512] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.27420971394\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:04 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:04 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_a219546d-096c-4ef4-93e8-daab6d030dd0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 132.88116455078125, \"sum\": 132.88116455078125, \"min\": 132.88116455078125}}, \"EndTime\": 1597163224.363488, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163224.230045}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:05 INFO 139712694728512] Epoch[2] Batch[0] avg_epoch_loss=3.061816\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.06181573868\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:06 INFO 139712694728512] Epoch[2] Batch[5] avg_epoch_loss=3.076424\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.07642388344\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:06 INFO 139712694728512] Epoch[2] Batch [5]#011Speed: 179.80 samples/sec#011loss=3.076424\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] Epoch[2] Batch[10] avg_epoch_loss=3.055618\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=3.03065152168\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] Epoch[2] Batch [10]#011Speed: 176.13 samples/sec#011loss=3.030652\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4411.406993865967, \"sum\": 4411.406993865967, \"min\": 4411.406993865967}}, \"EndTime\": 1597163228.775057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163224.363578}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.021071467 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.05561826446\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:08 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_95a5959c-492f-4e33-b35d-fe965d7359ef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 143.06211471557617, \"sum\": 143.06211471557617, \"min\": 143.06211471557617}}, \"EndTime\": 1597163228.918783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163228.775144}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:09 INFO 139712694728512] Epoch[3] Batch[0] avg_epoch_loss=2.952748\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=2.95274758339\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:11 INFO 139712694728512] Epoch[3] Batch[5] avg_epoch_loss=2.997583\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=2.99758267403\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:11 INFO 139712694728512] Epoch[3] Batch [5]#011Speed: 179.09 samples/sec#011loss=2.997583\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] Epoch[3] Batch[10] avg_epoch_loss=3.031796\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=3.07285223007\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] Epoch[3] Batch [10]#011Speed: 179.86 samples/sec#011loss=3.072852\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4397.244930267334, \"sum\": 4397.244930267334, \"min\": 4397.244930267334}}, \"EndTime\": 1597163233.316167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163228.918853}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.317052752 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.03179610859\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:13 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_58f16b8e-ac03-44e9-a74d-35b67db043fb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.83422660827637, \"sum\": 125.83422660827637, \"min\": 125.83422660827637}}, \"EndTime\": 1597163233.442591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163233.316256}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:14 INFO 139712694728512] Epoch[4] Batch[0] avg_epoch_loss=2.976892\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=2.97689247131\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:16 INFO 139712694728512] Epoch[4] Batch[5] avg_epoch_loss=2.850577\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=2.85057707628\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:16 INFO 139712694728512] Epoch[4] Batch [5]#011Speed: 180.45 samples/sec#011loss=2.850577\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:17 INFO 139712694728512] processed a total of 571 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3678.3950328826904, \"sum\": 3678.3950328826904, \"min\": 3678.3950328826904}}, \"EndTime\": 1597163237.121143, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163233.442677}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:17 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.225512866 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:17 INFO 139712694728512] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=4, train loss <loss>=2.85377457407\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:17 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:17 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_1a718800-5ef2-454f-957b-98995c90df74-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.42496490478516, \"sum\": 117.42496490478516, \"min\": 117.42496490478516}}, \"EndTime\": 1597163237.239216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163237.121225}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:18 INFO 139712694728512] Epoch[5] Batch[0] avg_epoch_loss=2.687168\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=2.6871676445\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:19 INFO 139712694728512] Epoch[5] Batch[5] avg_epoch_loss=2.720136\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=2.72013616562\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:19 INFO 139712694728512] Epoch[5] Batch [5]#011Speed: 180.21 samples/sec#011loss=2.720136\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] Epoch[5] Batch[10] avg_epoch_loss=2.644081\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=2.55281572342\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] Epoch[5] Batch [10]#011Speed: 171.83 samples/sec#011loss=2.552816\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4451.977968215942, \"sum\": 4451.977968215942, \"min\": 4451.977968215942}}, \"EndTime\": 1597163241.691354, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163237.239307}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.266100851 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=5, train loss <loss>=2.64408141916\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:21 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_0dcb88d7-f925-4d9f-82e6-54bf371429bf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 130.82194328308105, \"sum\": 130.82194328308105, \"min\": 130.82194328308105}}, \"EndTime\": 1597163241.822821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163241.691441}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:22 INFO 139712694728512] Epoch[6] Batch[0] avg_epoch_loss=2.749756\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=2.74975585938\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:24 INFO 139712694728512] Epoch[6] Batch[5] avg_epoch_loss=2.638972\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=2.63897204399\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:24 INFO 139712694728512] Epoch[6] Batch [5]#011Speed: 178.29 samples/sec#011loss=2.638972\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:25 INFO 139712694728512] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4036.1931324005127, \"sum\": 4036.1931324005127, \"min\": 4036.1931324005127}}, \"EndTime\": 1597163245.859154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163241.822896}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:25 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.826215397 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:25 INFO 139712694728512] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=6, train loss <loss>=2.59308571815\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:25 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:25 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_22a0d78b-ba34-4ba8-8529-303dc46bfdfa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 116.10889434814453, \"sum\": 116.10889434814453, \"min\": 116.10889434814453}}, \"EndTime\": 1597163245.97593, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163245.859236}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:26 INFO 139712694728512] Epoch[7] Batch[0] avg_epoch_loss=2.720114\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=2.72011399269\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:28 INFO 139712694728512] Epoch[7] Batch[5] avg_epoch_loss=2.563823\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=2.56382314364\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:28 INFO 139712694728512] Epoch[7] Batch [5]#011Speed: 182.47 samples/sec#011loss=2.563823\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] Epoch[7] Batch[10] avg_epoch_loss=2.465939\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=2.34847817421\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] Epoch[7] Batch [10]#011Speed: 180.17 samples/sec#011loss=2.348478\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4340.487957000732, \"sum\": 4340.487957000732, \"min\": 4340.487957000732}}, \"EndTime\": 1597163250.316565, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163245.976007}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.895732025 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] #quality_metric: host=algo-1, epoch=7, train loss <loss>=2.46593906663\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:30 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_989baa61-93d6-4d79-b0aa-fddb0b1021eb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.23709106445312, \"sum\": 117.23709106445312, \"min\": 117.23709106445312}}, \"EndTime\": 1597163250.434392, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163250.316639}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:31 INFO 139712694728512] Epoch[8] Batch[0] avg_epoch_loss=2.336399\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=2.33639907837\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:32 INFO 139712694728512] Epoch[8] Batch[5] avg_epoch_loss=2.326855\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=2.32685518265\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:32 INFO 139712694728512] Epoch[8] Batch [5]#011Speed: 181.99 samples/sec#011loss=2.326855\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] Epoch[8] Batch[10] avg_epoch_loss=2.320564\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=2.31301403046\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] Epoch[8] Batch [10]#011Speed: 182.43 samples/sec#011loss=2.313014\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4315.945863723755, \"sum\": 4315.945863723755, \"min\": 4315.945863723755}}, \"EndTime\": 1597163254.750485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163250.434468}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.685160853 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] #quality_metric: host=algo-1, epoch=8, train loss <loss>=2.32056374983\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:34 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_6068b2ba-5971-46d1-8d56-92d328664460-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 131.26897811889648, \"sum\": 131.26897811889648, \"min\": 131.26897811889648}}, \"EndTime\": 1597163254.882356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163254.750571}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:35 INFO 139712694728512] Epoch[9] Batch[0] avg_epoch_loss=2.597894\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=2.5978937149\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:37 INFO 139712694728512] Epoch[9] Batch[5] avg_epoch_loss=2.361195\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=2.36119500796\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:37 INFO 139712694728512] Epoch[9] Batch [5]#011Speed: 178.51 samples/sec#011loss=2.361195\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] Epoch[9] Batch[10] avg_epoch_loss=2.315898\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=2.26154198647\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] Epoch[9] Batch [10]#011Speed: 178.53 samples/sec#011loss=2.261542\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] processed a total of 707 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4776.336908340454, \"sum\": 4776.336908340454, \"min\": 4776.336908340454}}, \"EndTime\": 1597163259.65883, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163254.882423}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.017261284 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=9, train loss <loss>=2.28360766172\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:39 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_c4095805-0f04-4803-a4d2-f8b5c48bcb3f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 139.84012603759766, \"sum\": 139.84012603759766, \"min\": 139.84012603759766}}, \"EndTime\": 1597163259.799273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163259.65892}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:40 INFO 139712694728512] Epoch[10] Batch[0] avg_epoch_loss=2.157409\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.15740919113\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:42 INFO 139712694728512] Epoch[10] Batch[5] avg_epoch_loss=2.313635\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.31363455455\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:42 INFO 139712694728512] Epoch[10] Batch [5]#011Speed: 179.38 samples/sec#011loss=2.313635\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] Epoch[10] Batch[10] avg_epoch_loss=2.281540\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=2.2430267334\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] Epoch[10] Batch [10]#011Speed: 180.32 samples/sec#011loss=2.243027\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4389.110088348389, \"sum\": 4389.110088348389, \"min\": 4389.110088348389}}, \"EndTime\": 1597163264.18854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163259.79936}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.228640097 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.28154009039\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:44 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_2bbbc85b-4859-4ee7-a6e7-8298ea521405-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 148.84114265441895, \"sum\": 148.84114265441895, \"min\": 148.84114265441895}}, \"EndTime\": 1597163264.337975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163264.188627}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:45 INFO 139712694728512] Epoch[11] Batch[0] avg_epoch_loss=2.330943\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.3309431076\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:46 INFO 139712694728512] Epoch[11] Batch[5] avg_epoch_loss=2.203883\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.20388265451\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:46 INFO 139712694728512] Epoch[11] Batch [5]#011Speed: 179.40 samples/sec#011loss=2.203883\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] Epoch[11] Batch[10] avg_epoch_loss=2.183568\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=2.15919139385\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] Epoch[11] Batch [10]#011Speed: 180.49 samples/sec#011loss=2.159191\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4377.937078475952, \"sum\": 4377.937078475952, \"min\": 4377.937078475952}}, \"EndTime\": 1597163268.716075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163264.338062}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.35100407 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.18356844512\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:48 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_a33a7f67-46ce-4651-a427-9f39cc521dd0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 118.01791191101074, \"sum\": 118.01791191101074, \"min\": 118.01791191101074}}, \"EndTime\": 1597163268.834699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163268.716144}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:49 INFO 139712694728512] Epoch[12] Batch[0] avg_epoch_loss=2.155762\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.15576195717\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:51 INFO 139712694728512] Epoch[12] Batch[5] avg_epoch_loss=2.136740\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.1367401282\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:51 INFO 139712694728512] Epoch[12] Batch [5]#011Speed: 179.96 samples/sec#011loss=2.136740\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:52 INFO 139712694728512] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4028.9831161499023, \"sum\": 4028.9831161499023, \"min\": 4028.9831161499023}}, \"EndTime\": 1597163272.86383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163268.834773}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:52 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.106341985 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:52 INFO 139712694728512] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.12231469154\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:52 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:52 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_570a89e3-3751-4801-9136-b9f8e632ced7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 126.94311141967773, \"sum\": 126.94311141967773, \"min\": 126.94311141967773}}, \"EndTime\": 1597163272.991422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163272.863924}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:53 INFO 139712694728512] Epoch[13] Batch[0] avg_epoch_loss=2.084434\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:53 INFO 139712694728512] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.08443403244\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:55 INFO 139712694728512] Epoch[13] Batch[5] avg_epoch_loss=2.046479\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.04647884766\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:55 INFO 139712694728512] Epoch[13] Batch [5]#011Speed: 178.97 samples/sec#011loss=2.046479\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4025.02703666687, \"sum\": 4025.02703666687, \"min\": 4025.02703666687}}, \"EndTime\": 1597163277.016613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163272.991525}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.763961119 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.02522864342\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_9dd3a2f3-6d9a-4ac7-ad59-d17bc672ad17-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 127.66504287719727, \"sum\": 127.66504287719727, \"min\": 127.66504287719727}}, \"EndTime\": 1597163277.144908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163277.016707}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] Epoch[14] Batch[0] avg_epoch_loss=1.979722\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.97972249985\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:59 INFO 139712694728512] Epoch[14] Batch[5] avg_epoch_loss=2.038990\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.03898994128\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:27:59 INFO 139712694728512] Epoch[14] Batch [5]#011Speed: 180.09 samples/sec#011loss=2.038990\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:01 INFO 139712694728512] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4048.8460063934326, \"sum\": 4048.8460063934326, \"min\": 4048.8460063934326}}, \"EndTime\": 1597163281.193887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163277.144979}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:01 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.088739989 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:01 INFO 139712694728512] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.05711859465\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:01 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:02 INFO 139712694728512] Epoch[15] Batch[0] avg_epoch_loss=2.001461\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.00146055222\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:03 INFO 139712694728512] Epoch[15] Batch[5] avg_epoch_loss=1.956472\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.95647172133\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:03 INFO 139712694728512] Epoch[15] Batch [5]#011Speed: 181.33 samples/sec#011loss=1.956472\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:05 INFO 139712694728512] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4028.377056121826, \"sum\": 4028.377056121826, \"min\": 4028.377056121826}}, \"EndTime\": 1597163285.222841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163281.193981}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:05 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.633636766 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:05 INFO 139712694728512] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.97117103338\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:05 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:05 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_d95b136c-99b8-4178-8f2b-94cc215268e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 136.87896728515625, \"sum\": 136.87896728515625, \"min\": 136.87896728515625}}, \"EndTime\": 1597163285.36035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163285.222934}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:06 INFO 139712694728512] Epoch[16] Batch[0] avg_epoch_loss=1.857983\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.85798323154\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:07 INFO 139712694728512] Epoch[16] Batch[5] avg_epoch_loss=1.939051\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=1.93905133009\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:07 INFO 139712694728512] Epoch[16] Batch [5]#011Speed: 181.99 samples/sec#011loss=1.939051\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:09 INFO 139712694728512] Epoch[16] Batch[10] avg_epoch_loss=1.952513\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=1.96866619587\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:09 INFO 139712694728512] Epoch[16] Batch [10]#011Speed: 178.36 samples/sec#011loss=1.968666\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:10 INFO 139712694728512] processed a total of 710 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4772.156000137329, \"sum\": 4772.156000137329, \"min\": 4772.156000137329}}, \"EndTime\": 1597163290.132641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163285.360422}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:10 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.775505973 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:10 INFO 139712694728512] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=16, train loss <loss>=1.99683868885\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:10 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:11 INFO 139712694728512] Epoch[17] Batch[0] avg_epoch_loss=1.867600\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.86759996414\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:12 INFO 139712694728512] Epoch[17] Batch[5] avg_epoch_loss=1.889373\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.88937280575\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:12 INFO 139712694728512] Epoch[17] Batch [5]#011Speed: 180.78 samples/sec#011loss=1.889373\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:14 INFO 139712694728512] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4053.9660453796387, \"sum\": 4053.9660453796387, \"min\": 4053.9660453796387}}, \"EndTime\": 1597163294.187206, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163290.132736}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:14 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.398118611 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:14 INFO 139712694728512] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=17, train loss <loss>=1.90540349483\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:14 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:14 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_c3bf42a4-7afe-47d9-863d-c2a56021e46a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 115.76080322265625, \"sum\": 115.76080322265625, \"min\": 115.76080322265625}}, \"EndTime\": 1597163294.303677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163294.187302}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:15 INFO 139712694728512] Epoch[18] Batch[0] avg_epoch_loss=1.918692\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.91869211197\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:16 INFO 139712694728512] Epoch[18] Batch[5] avg_epoch_loss=1.937403\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=1.93740328153\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:16 INFO 139712694728512] Epoch[18] Batch [5]#011Speed: 179.89 samples/sec#011loss=1.937403\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:18 INFO 139712694728512] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4019.5460319519043, \"sum\": 4019.5460319519043, \"min\": 4019.5460319519043}}, \"EndTime\": 1597163298.323356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163294.303748}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:18 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.992393247 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:18 INFO 139712694728512] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=18, train loss <loss>=1.90622090101\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:18 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:19 INFO 139712694728512] Epoch[19] Batch[0] avg_epoch_loss=1.946721\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=1.94672143459\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:20 INFO 139712694728512] Epoch[19] Batch[5] avg_epoch_loss=1.885214\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.88521422942\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:20 INFO 139712694728512] Epoch[19] Batch [5]#011Speed: 179.43 samples/sec#011loss=1.885214\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:22 INFO 139712694728512] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4019.2298889160156, \"sum\": 4019.2298889160156, \"min\": 4019.2298889160156}}, \"EndTime\": 1597163302.343254, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163298.32345}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:22 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.492341315 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:22 INFO 139712694728512] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.87030799389\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:22 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:22 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_5746c8b6-eab1-46ab-9f5f-74b72bdf454a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.54407119750977, \"sum\": 125.54407119750977, \"min\": 125.54407119750977}}, \"EndTime\": 1597163302.469462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163302.343347}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:23 INFO 139712694728512] Epoch[20] Batch[0] avg_epoch_loss=1.837368\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=1.8373683691\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:25 INFO 139712694728512] Epoch[20] Batch[5] avg_epoch_loss=1.804316\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.80431570609\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:25 INFO 139712694728512] Epoch[20] Batch [5]#011Speed: 180.28 samples/sec#011loss=1.804316\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:26 INFO 139712694728512] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4022.306203842163, \"sum\": 4022.306203842163, \"min\": 4022.306203842163}}, \"EndTime\": 1597163306.491915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163302.469548}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:26 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.649734739 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:26 INFO 139712694728512] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.81939474344\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:26 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:26 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_f5ae0bfe-7c73-48bf-afc0-d9704d9bbfd8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.78402328491211, \"sum\": 117.78402328491211, \"min\": 117.78402328491211}}, \"EndTime\": 1597163306.610336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163306.491995}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:27 INFO 139712694728512] Epoch[21] Batch[0] avg_epoch_loss=2.011372\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.0113723278\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:29 INFO 139712694728512] Epoch[21] Batch[5] avg_epoch_loss=1.856174\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=1.85617361466\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:29 INFO 139712694728512] Epoch[21] Batch [5]#011Speed: 181.62 samples/sec#011loss=1.856174\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] Epoch[21] Batch[10] avg_epoch_loss=1.800467\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=1.73361914158\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] Epoch[21] Batch [10]#011Speed: 179.75 samples/sec#011loss=1.733619\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4365.276098251343, \"sum\": 4365.276098251343, \"min\": 4365.276098251343}}, \"EndTime\": 1597163310.975768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163306.610421}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.836524464 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] #quality_metric: host=algo-1, epoch=21, train loss <loss>=1.80046703599\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:30 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:31 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_7ef08b2f-3a1f-444a-b118-d3c7ddc0c6df-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 136.45482063293457, \"sum\": 136.45482063293457, \"min\": 136.45482063293457}}, \"EndTime\": 1597163311.112788, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163310.975852}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:31 INFO 139712694728512] Epoch[22] Batch[0] avg_epoch_loss=1.772418\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=1.77241754532\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:33 INFO 139712694728512] Epoch[22] Batch[5] avg_epoch_loss=1.815900\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.81589967012\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:33 INFO 139712694728512] Epoch[22] Batch [5]#011Speed: 180.95 samples/sec#011loss=1.815900\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:35 INFO 139712694728512] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3998.821020126343, \"sum\": 3998.821020126343, \"min\": 3998.821020126343}}, \"EndTime\": 1597163315.111743, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163311.112853}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:35 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.040811722 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:35 INFO 139712694728512] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.79167398214\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:35 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:35 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_84305a0d-fc58-4893-aea9-b6c7af4258ef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 129.13012504577637, \"sum\": 129.13012504577637, \"min\": 129.13012504577637}}, \"EndTime\": 1597163315.241501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163315.111836}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:36 INFO 139712694728512] Epoch[23] Batch[0] avg_epoch_loss=1.807313\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=1.80731320381\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:37 INFO 139712694728512] Epoch[23] Batch[5] avg_epoch_loss=1.827670\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=1.82766975959\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:37 INFO 139712694728512] Epoch[23] Batch [5]#011Speed: 180.74 samples/sec#011loss=1.827670\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:39 INFO 139712694728512] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4043.7400341033936, \"sum\": 4043.7400341033936, \"min\": 4043.7400341033936}}, \"EndTime\": 1597163319.285403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163315.24159}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:39 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.027520967 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:39 INFO 139712694728512] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=23, train loss <loss>=1.79884693623\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:39 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:40 INFO 139712694728512] Epoch[24] Batch[0] avg_epoch_loss=1.776466\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=1.77646553516\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:41 INFO 139712694728512] Epoch[24] Batch[5] avg_epoch_loss=1.726582\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=1.72658177217\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:41 INFO 139712694728512] Epoch[24] Batch [5]#011Speed: 176.94 samples/sec#011loss=1.726582\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:43 INFO 139712694728512] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4065.6468868255615, \"sum\": 4065.6468868255615, \"min\": 4065.6468868255615}}, \"EndTime\": 1597163323.351688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163319.285497}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:43 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.6734612 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:43 INFO 139712694728512] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.71569652557\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:43 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:43 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_7d1d490c-adf3-42bb-8ac4-476a20c4913b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 126.56593322753906, \"sum\": 126.56593322753906, \"min\": 126.56593322753906}}, \"EndTime\": 1597163323.478883, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163323.351782}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:44 INFO 139712694728512] Epoch[25] Batch[0] avg_epoch_loss=1.691504\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=1.69150412083\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:46 INFO 139712694728512] Epoch[25] Batch[5] avg_epoch_loss=1.667353\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=1.6673531731\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:46 INFO 139712694728512] Epoch[25] Batch [5]#011Speed: 178.84 samples/sec#011loss=1.667353\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] Epoch[25] Batch[10] avg_epoch_loss=1.714085\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=1.77016210556\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] Epoch[25] Batch [10]#011Speed: 179.92 samples/sec#011loss=1.770162\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4407.782077789307, \"sum\": 4407.782077789307, \"min\": 4407.782077789307}}, \"EndTime\": 1597163327.886827, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163323.47897}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.453199691 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] #quality_metric: host=algo-1, epoch=25, train loss <loss>=1.71408450603\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:47 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:48 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_5269fffc-d9f4-4123-9fe8-78d5b577718f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 119.20022964477539, \"sum\": 119.20022964477539, \"min\": 119.20022964477539}}, \"EndTime\": 1597163328.006675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163327.886914}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:48 INFO 139712694728512] Epoch[26] Batch[0] avg_epoch_loss=1.730890\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=1.730889678\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:50 INFO 139712694728512] Epoch[26] Batch[5] avg_epoch_loss=1.748908\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=1.74890830119\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:50 INFO 139712694728512] Epoch[26] Batch [5]#011Speed: 178.86 samples/sec#011loss=1.748908\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4028.640031814575, \"sum\": 4028.640031814575, \"min\": 4028.640031814575}}, \"EndTime\": 1597163332.035453, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163328.006752}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.643264404 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=26, train loss <loss>=1.69731612206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_5d4644e6-b55e-48d0-9cd3-bfef26599945-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 126.92594528198242, \"sum\": 126.92594528198242, \"min\": 126.92594528198242}}, \"EndTime\": 1597163332.163069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163332.035585}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] Epoch[27] Batch[0] avg_epoch_loss=1.750513\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=1.75051295757\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:54 INFO 139712694728512] Epoch[27] Batch[5] avg_epoch_loss=1.743376\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.74337571859\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:54 INFO 139712694728512] Epoch[27] Batch [5]#011Speed: 180.76 samples/sec#011loss=1.743376\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:56 INFO 139712694728512] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4058.2170486450195, \"sum\": 4058.2170486450195, \"min\": 4058.2170486450195}}, \"EndTime\": 1597163336.221446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163332.163158}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:56 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.336073597 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:56 INFO 139712694728512] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.77237159014\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:56 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:57 INFO 139712694728512] Epoch[28] Batch[0] avg_epoch_loss=1.631096\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.6310955286\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:58 INFO 139712694728512] Epoch[28] Batch[5] avg_epoch_loss=1.648206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.64820591609\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:28:58 INFO 139712694728512] Epoch[28] Batch [5]#011Speed: 183.85 samples/sec#011loss=1.648206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:00 INFO 139712694728512] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3995.407819747925, \"sum\": 3995.407819747925, \"min\": 3995.407819747925}}, \"EndTime\": 1597163340.217394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163336.221539}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:00 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.676843382 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:00 INFO 139712694728512] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.65343469381\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:00 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:00 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_64d9f4b7-65ea-41dd-80b6-036e03d003dc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.33712387084961, \"sum\": 125.33712387084961, \"min\": 125.33712387084961}}, \"EndTime\": 1597163340.34338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163340.217488}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:01 INFO 139712694728512] Epoch[29] Batch[0] avg_epoch_loss=1.676197\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=1.67619693279\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:02 INFO 139712694728512] Epoch[29] Batch[5] avg_epoch_loss=1.639945\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=1.63994475206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:02 INFO 139712694728512] Epoch[29] Batch [5]#011Speed: 181.82 samples/sec#011loss=1.639945\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] Epoch[29] Batch[10] avg_epoch_loss=1.591171\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=1.53264334202\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] Epoch[29] Batch [10]#011Speed: 183.81 samples/sec#011loss=1.532643\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4373.380184173584, \"sum\": 4373.380184173584, \"min\": 4373.380184173584}}, \"EndTime\": 1597163344.716919, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163340.343454}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.393601873 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=29, train loss <loss>=1.59117138386\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:04 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_78f7cb15-01f7-4c98-b1ec-7c95239ba94e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 122.63298034667969, \"sum\": 122.63298034667969, \"min\": 122.63298034667969}}, \"EndTime\": 1597163344.840145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163344.717006}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:05 INFO 139712694728512] Epoch[30] Batch[0] avg_epoch_loss=1.624872\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=1.62487220764\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:07 INFO 139712694728512] Epoch[30] Batch[5] avg_epoch_loss=1.633074\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=1.63307446241\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:07 INFO 139712694728512] Epoch[30] Batch [5]#011Speed: 179.80 samples/sec#011loss=1.633074\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:08 INFO 139712694728512] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4014.820098876953, \"sum\": 4014.820098876953, \"min\": 4014.820098876953}}, \"EndTime\": 1597163348.855086, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163344.840209}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:08 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.416250545 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:08 INFO 139712694728512] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=30, train loss <loss>=1.61917257309\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:08 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:09 INFO 139712694728512] Epoch[31] Batch[0] avg_epoch_loss=1.499224\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=1.4992235899\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:11 INFO 139712694728512] Epoch[31] Batch[5] avg_epoch_loss=1.583663\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.58366344372\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:11 INFO 139712694728512] Epoch[31] Batch [5]#011Speed: 177.24 samples/sec#011loss=1.583663\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] Epoch[31] Batch[10] avg_epoch_loss=1.556545\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=1.52400186062\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] Epoch[31] Batch [10]#011Speed: 180.66 samples/sec#011loss=1.524002\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4418.045997619629, \"sum\": 4418.045997619629, \"min\": 4418.045997619629}}, \"EndTime\": 1597163353.273832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163348.855159}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.267696438 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.55654454231\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:13 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_ca618300-c142-4d67-b4fa-d3cebd40f692-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.05708503723145, \"sum\": 117.05708503723145, \"min\": 117.05708503723145}}, \"EndTime\": 1597163353.391542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163353.27392}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:14 INFO 139712694728512] Epoch[32] Batch[0] avg_epoch_loss=1.584849\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=1.58484947681\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:15 INFO 139712694728512] Epoch[32] Batch[5] avg_epoch_loss=1.546789\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=1.5467890501\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:15 INFO 139712694728512] Epoch[32] Batch [5]#011Speed: 179.35 samples/sec#011loss=1.546789\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] Epoch[32] Batch[10] avg_epoch_loss=1.581619\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=1.62341601849\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] Epoch[32] Batch [10]#011Speed: 178.22 samples/sec#011loss=1.623416\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4396.347999572754, \"sum\": 4396.347999572754, \"min\": 4396.347999572754}}, \"EndTime\": 1597163357.788038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163353.391629}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.983740834 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=32, train loss <loss>=1.58161949028\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:17 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:18 INFO 139712694728512] Epoch[33] Batch[0] avg_epoch_loss=1.508749\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=1.50874888897\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:20 INFO 139712694728512] Epoch[33] Batch[5] avg_epoch_loss=1.549164\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=1.54916363955\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:20 INFO 139712694728512] Epoch[33] Batch [5]#011Speed: 178.05 samples/sec#011loss=1.549164\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] Epoch[33] Batch[10] avg_epoch_loss=1.541076\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=1.5313713789\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] Epoch[33] Batch [10]#011Speed: 179.45 samples/sec#011loss=1.531371\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4403.903007507324, \"sum\": 4403.903007507324, \"min\": 4403.903007507324}}, \"EndTime\": 1597163362.192513, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163357.788104}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.495947016 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=33, train loss <loss>=1.54107624834\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:22 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_bab50bdc-f58b-431c-a134-48c3fc2f2b0c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.91610717773438, \"sum\": 117.91610717773438, \"min\": 117.91610717773438}}, \"EndTime\": 1597163362.311022, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163362.192594}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:23 INFO 139712694728512] Epoch[34] Batch[0] avg_epoch_loss=1.448733\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=1.44873332977\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:24 INFO 139712694728512] Epoch[34] Batch[5] avg_epoch_loss=1.525151\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=1.52515053749\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:24 INFO 139712694728512] Epoch[34] Batch [5]#011Speed: 181.24 samples/sec#011loss=1.525151\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] Epoch[34] Batch[10] avg_epoch_loss=1.460583\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=1.38310295343\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] Epoch[34] Batch [10]#011Speed: 177.77 samples/sec#011loss=1.383103\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4382.081985473633, \"sum\": 4382.081985473633, \"min\": 4382.081985473633}}, \"EndTime\": 1597163366.693255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163362.311103}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.555350239 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=34, train loss <loss>=1.46058345383\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:26 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_4cab98e4-afee-45bc-8f97-5fff0515cbef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 116.47891998291016, \"sum\": 116.47891998291016, \"min\": 116.47891998291016}}, \"EndTime\": 1597163366.810395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163366.693336}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:27 INFO 139712694728512] Epoch[35] Batch[0] avg_epoch_loss=1.649366\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=1.64936602116\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:29 INFO 139712694728512] Epoch[35] Batch[5] avg_epoch_loss=1.550828\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=1.55082770189\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:29 INFO 139712694728512] Epoch[35] Batch [5]#011Speed: 181.22 samples/sec#011loss=1.550828\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] Epoch[35] Batch[10] avg_epoch_loss=1.761211\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=2.01367061138\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] Epoch[35] Batch [10]#011Speed: 176.21 samples/sec#011loss=2.013671\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4397.040128707886, \"sum\": 4397.040128707886, \"min\": 4397.040128707886}}, \"EndTime\": 1597163371.207595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163366.810484}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.186764033 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=35, train loss <loss>=1.76121084257\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:31 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:32 INFO 139712694728512] Epoch[36] Batch[0] avg_epoch_loss=1.552124\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=1.55212414265\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:33 INFO 139712694728512] Epoch[36] Batch[5] avg_epoch_loss=1.492692\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=1.4926923116\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:33 INFO 139712694728512] Epoch[36] Batch [5]#011Speed: 181.34 samples/sec#011loss=1.492692\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] Epoch[36] Batch[10] avg_epoch_loss=1.575852\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=1.67564349174\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] Epoch[36] Batch [10]#011Speed: 179.23 samples/sec#011loss=1.675643\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4375.821828842163, \"sum\": 4375.821828842163, \"min\": 4375.821828842163}}, \"EndTime\": 1597163375.58397, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163371.207685}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.367623984 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=36, train loss <loss>=1.57585193894\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:35 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:36 INFO 139712694728512] Epoch[37] Batch[0] avg_epoch_loss=1.712724\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=1.71272420883\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:38 INFO 139712694728512] Epoch[37] Batch[5] avg_epoch_loss=1.530256\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=1.53025555611\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:38 INFO 139712694728512] Epoch[37] Batch [5]#011Speed: 180.92 samples/sec#011loss=1.530256\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] Epoch[37] Batch[10] avg_epoch_loss=1.475806\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=1.41046651602\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] Epoch[37] Batch [10]#011Speed: 177.89 samples/sec#011loss=1.410467\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4385.026931762695, \"sum\": 4385.026931762695, \"min\": 4385.026931762695}}, \"EndTime\": 1597163379.969553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163375.584049}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.631476761 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=37, train loss <loss>=1.47580599243\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:39 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:40 INFO 139712694728512] Epoch[38] Batch[0] avg_epoch_loss=1.826396\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=1.82639575005\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:42 INFO 139712694728512] Epoch[38] Batch[5] avg_epoch_loss=1.578422\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=1.5784222285\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:42 INFO 139712694728512] Epoch[38] Batch [5]#011Speed: 179.79 samples/sec#011loss=1.578422\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:43 INFO 139712694728512] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4010.8108520507812, \"sum\": 4010.8108520507812, \"min\": 4010.8108520507812}}, \"EndTime\": 1597163383.980873, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163379.969633}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:43 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=159.314708101 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:43 INFO 139712694728512] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=38, train loss <loss>=1.54578331709\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:43 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:44 INFO 139712694728512] Epoch[39] Batch[0] avg_epoch_loss=1.398301\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=1.39830136299\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:46 INFO 139712694728512] Epoch[39] Batch[5] avg_epoch_loss=1.456881\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=1.45688052972\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:46 INFO 139712694728512] Epoch[39] Batch [5]#011Speed: 179.25 samples/sec#011loss=1.456881\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] Epoch[39] Batch[10] avg_epoch_loss=1.515902\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=1.58672707081\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] Epoch[39] Batch [10]#011Speed: 181.74 samples/sec#011loss=1.586727\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4375.993967056274, \"sum\": 4375.993967056274, \"min\": 4375.993967056274}}, \"EndTime\": 1597163388.35744, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163383.980957}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.759866232 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=39, train loss <loss>=1.51590168476\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:48 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:49 INFO 139712694728512] Epoch[40] Batch[0] avg_epoch_loss=1.524685\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=1.5246847868\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:50 INFO 139712694728512] Epoch[40] Batch[5] avg_epoch_loss=1.412396\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=1.41239575545\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:50 INFO 139712694728512] Epoch[40] Batch [5]#011Speed: 178.65 samples/sec#011loss=1.412396\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] Epoch[40] Batch[10] avg_epoch_loss=1.375802\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=1.33188853264\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] Epoch[40] Batch [10]#011Speed: 179.21 samples/sec#011loss=1.331889\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4427.613973617554, \"sum\": 4427.613973617554, \"min\": 4427.613973617554}}, \"EndTime\": 1597163392.785578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163388.357527}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.286160851 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.37580156326\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:52 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_3e6f89c1-2055-4c18-b296-cb88b5c0839d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 124.1459846496582, \"sum\": 124.1459846496582, \"min\": 124.1459846496582}}, \"EndTime\": 1597163392.910322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163392.785664}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:53 INFO 139712694728512] Epoch[41] Batch[0] avg_epoch_loss=1.520131\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:53 INFO 139712694728512] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=1.52013063431\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:55 INFO 139712694728512] Epoch[41] Batch[5] avg_epoch_loss=1.452071\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=1.45207101107\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:55 INFO 139712694728512] Epoch[41] Batch [5]#011Speed: 178.52 samples/sec#011loss=1.452071\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] Epoch[41] Batch[10] avg_epoch_loss=1.439204\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=1.4237641573\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] Epoch[41] Batch [10]#011Speed: 174.07 samples/sec#011loss=1.423764\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4451.758146286011, \"sum\": 4451.758146286011, \"min\": 4451.758146286011}}, \"EndTime\": 1597163397.362248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163392.91042}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.353415223 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=41, train loss <loss>=1.43920425935\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:57 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:58 INFO 139712694728512] Epoch[42] Batch[0] avg_epoch_loss=1.405000\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:29:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=1.40500044823\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:00 INFO 139712694728512] Epoch[42] Batch[5] avg_epoch_loss=1.399632\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=1.39963171879\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:00 INFO 139712694728512] Epoch[42] Batch [5]#011Speed: 179.58 samples/sec#011loss=1.399632\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:01 INFO 139712694728512] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4146.666049957275, \"sum\": 4146.666049957275, \"min\": 4146.666049957275}}, \"EndTime\": 1597163401.509423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163397.362333}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:01 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.130202149 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:01 INFO 139712694728512] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=42, train loss <loss>=1.43448437452\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:01 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:02 INFO 139712694728512] Epoch[43] Batch[0] avg_epoch_loss=1.368360\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=1.36835956573\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:04 INFO 139712694728512] Epoch[43] Batch[5] avg_epoch_loss=1.431904\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=1.43190395832\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:04 INFO 139712694728512] Epoch[43] Batch [5]#011Speed: 181.16 samples/sec#011loss=1.431904\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:05 INFO 139712694728512] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4067.434072494507, \"sum\": 4067.434072494507, \"min\": 4067.434072494507}}, \"EndTime\": 1597163405.577438, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163401.509516}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:05 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.737968013 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:05 INFO 139712694728512] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=43, train loss <loss>=1.44989435673\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:05 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:06 INFO 139712694728512] Epoch[44] Batch[0] avg_epoch_loss=1.480110\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=1.48011028767\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:08 INFO 139712694728512] Epoch[44] Batch[5] avg_epoch_loss=1.364804\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=1.36480404933\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:08 INFO 139712694728512] Epoch[44] Batch [5]#011Speed: 179.51 samples/sec#011loss=1.364804\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:09 INFO 139712694728512] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4033.5819721221924, \"sum\": 4033.5819721221924, \"min\": 4033.5819721221924}}, \"EndTime\": 1597163409.611652, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163405.577516}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:09 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.456407585 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:09 INFO 139712694728512] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=44, train loss <loss>=1.38106694221\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:09 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:10 INFO 139712694728512] Epoch[45] Batch[0] avg_epoch_loss=1.216845\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.21684527397\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:12 INFO 139712694728512] Epoch[45] Batch[5] avg_epoch_loss=1.394069\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=1.39406861862\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:12 INFO 139712694728512] Epoch[45] Batch [5]#011Speed: 174.52 samples/sec#011loss=1.394069\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] Epoch[45] Batch[10] avg_epoch_loss=1.354066\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=1.30606215\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] Epoch[45] Batch [10]#011Speed: 181.46 samples/sec#011loss=1.306062\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4439.189910888672, \"sum\": 4439.189910888672, \"min\": 4439.189910888672}}, \"EndTime\": 1597163414.051419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163409.611748}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.247523167 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=45, train loss <loss>=1.35406567834\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:14 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_f09735f9-776d-4b0c-9183-73fb8c5059e7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.99396705627441, \"sum\": 125.99396705627441, \"min\": 125.99396705627441}}, \"EndTime\": 1597163414.178041, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163414.051531}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:15 INFO 139712694728512] Epoch[46] Batch[0] avg_epoch_loss=1.368162\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.36816227436\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:16 INFO 139712694728512] Epoch[46] Batch[5] avg_epoch_loss=1.356711\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=1.35671114922\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:16 INFO 139712694728512] Epoch[46] Batch [5]#011Speed: 180.41 samples/sec#011loss=1.356711\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:18 INFO 139712694728512] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4015.7859325408936, \"sum\": 4015.7859325408936, \"min\": 4015.7859325408936}}, \"EndTime\": 1597163418.193989, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163414.178128}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:18 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.618587591 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:18 INFO 139712694728512] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=46, train loss <loss>=1.3628446579\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:18 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:19 INFO 139712694728512] Epoch[47] Batch[0] avg_epoch_loss=1.356142\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=1.35614180565\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:20 INFO 139712694728512] Epoch[47] Batch[5] avg_epoch_loss=1.400689\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=1.40068920453\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:20 INFO 139712694728512] Epoch[47] Batch [5]#011Speed: 180.75 samples/sec#011loss=1.400689\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] Epoch[47] Batch[10] avg_epoch_loss=1.339044\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=1.26507031918\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] Epoch[47] Batch [10]#011Speed: 180.11 samples/sec#011loss=1.265070\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4367.1228885650635, \"sum\": 4367.1228885650635, \"min\": 4367.1228885650635}}, \"EndTime\": 1597163422.56168, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163418.194083}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.919414524 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=47, train loss <loss>=1.33904425664\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:22 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_b530aa93-ed3c-4a7a-bea8-6afac8845726-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.34795570373535, \"sum\": 117.34795570373535, \"min\": 117.34795570373535}}, \"EndTime\": 1597163422.679616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163422.561765}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:23 INFO 139712694728512] Epoch[48] Batch[0] avg_epoch_loss=1.397707\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=1.39770662785\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:25 INFO 139712694728512] Epoch[48] Batch[5] avg_epoch_loss=1.373247\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=1.37324680885\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:25 INFO 139712694728512] Epoch[48] Batch [5]#011Speed: 181.88 samples/sec#011loss=1.373247\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] Epoch[48] Batch[10] avg_epoch_loss=1.387421\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=1.40443012714\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] Epoch[48] Batch [10]#011Speed: 180.06 samples/sec#011loss=1.404430\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4344.897985458374, \"sum\": 4344.897985458374, \"min\": 4344.897985458374}}, \"EndTime\": 1597163427.024652, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163422.679696}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.207487873 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=48, train loss <loss>=1.38742104444\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] Epoch[49] Batch[0] avg_epoch_loss=1.363390\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=1.36339044571\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:29 INFO 139712694728512] Epoch[49] Batch[5] avg_epoch_loss=1.355384\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=1.35538440943\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:29 INFO 139712694728512] Epoch[49] Batch [5]#011Speed: 183.47 samples/sec#011loss=1.355384\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] Epoch[49] Batch[10] avg_epoch_loss=1.389918\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=1.43135797977\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] Epoch[49] Batch [10]#011Speed: 178.63 samples/sec#011loss=1.431358\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4368.973016738892, \"sum\": 4368.973016738892, \"min\": 4368.973016738892}}, \"EndTime\": 1597163431.394156, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163427.02474}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.459143951 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=49, train loss <loss>=1.38991785049\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:31 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:32 INFO 139712694728512] Epoch[50] Batch[0] avg_epoch_loss=1.534826\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=1.53482615948\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:33 INFO 139712694728512] Epoch[50] Batch[5] avg_epoch_loss=1.286417\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=1.2864172856\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:33 INFO 139712694728512] Epoch[50] Batch [5]#011Speed: 180.86 samples/sec#011loss=1.286417\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] Epoch[50] Batch[10] avg_epoch_loss=1.553364\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=1.87369990349\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] Epoch[50] Batch [10]#011Speed: 179.45 samples/sec#011loss=1.873700\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4362.661838531494, \"sum\": 4362.661838531494, \"min\": 4362.661838531494}}, \"EndTime\": 1597163435.757391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163431.394229}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.29969897 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=50, train loss <loss>=1.5533639301\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:35 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:36 INFO 139712694728512] Epoch[51] Batch[0] avg_epoch_loss=1.341982\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=1.34198188782\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:38 INFO 139712694728512] Epoch[51] Batch[5] avg_epoch_loss=1.377822\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=1.37782156467\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:38 INFO 139712694728512] Epoch[51] Batch [5]#011Speed: 180.73 samples/sec#011loss=1.377822\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] Epoch[51] Batch[10] avg_epoch_loss=1.307290\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=1.22265278101\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] Epoch[51] Batch [10]#011Speed: 179.02 samples/sec#011loss=1.222653\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4392.7600383758545, \"sum\": 4392.7600383758545, \"min\": 4392.7600383758545}}, \"EndTime\": 1597163440.150712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163435.757473}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.787945451 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=51, train loss <loss>=1.30729029937\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:40 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_956c746e-aff5-4e41-bd8b-fbae19335fc3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 115.27013778686523, \"sum\": 115.27013778686523, \"min\": 115.27013778686523}}, \"EndTime\": 1597163440.266637, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163440.150792}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:41 INFO 139712694728512] Epoch[52] Batch[0] avg_epoch_loss=1.250851\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=1.25085103512\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:42 INFO 139712694728512] Epoch[52] Batch[5] avg_epoch_loss=1.344100\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=1.34409993887\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:42 INFO 139712694728512] Epoch[52] Batch [5]#011Speed: 177.75 samples/sec#011loss=1.344100\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] Epoch[52] Batch[10] avg_epoch_loss=1.329331\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=1.31160886288\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] Epoch[52] Batch [10]#011Speed: 179.63 samples/sec#011loss=1.311609\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4442.2619342803955, \"sum\": 4442.2619342803955, \"min\": 4442.2619342803955}}, \"EndTime\": 1597163444.709062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163440.266724}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.018981124 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=52, train loss <loss>=1.32933126796\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:44 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:45 INFO 139712694728512] Epoch[53] Batch[0] avg_epoch_loss=1.459200\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=1.4592000246\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:47 INFO 139712694728512] Epoch[53] Batch[5] avg_epoch_loss=1.370804\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:47 INFO 139712694728512] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=1.37080409129\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:47 INFO 139712694728512] Epoch[53] Batch [5]#011Speed: 176.24 samples/sec#011loss=1.370804\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:48 INFO 139712694728512] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4036.80682182312, \"sum\": 4036.80682182312, \"min\": 4036.80682182312}}, \"EndTime\": 1597163448.746405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163444.709144}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:48 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.306662532 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:48 INFO 139712694728512] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=53, train loss <loss>=1.3312505722\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:48 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:49 INFO 139712694728512] Epoch[54] Batch[0] avg_epoch_loss=1.275521\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=1.27552127838\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:51 INFO 139712694728512] Epoch[54] Batch[5] avg_epoch_loss=1.340356\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=1.34035555522\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:51 INFO 139712694728512] Epoch[54] Batch [5]#011Speed: 181.27 samples/sec#011loss=1.340356\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:52 INFO 139712694728512] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4053.230047225952, \"sum\": 4053.230047225952, \"min\": 4053.230047225952}}, \"EndTime\": 1597163452.800175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163448.746495}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:52 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.179782033 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:52 INFO 139712694728512] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=54, train loss <loss>=1.32401294708\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:52 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:53 INFO 139712694728512] Epoch[55] Batch[0] avg_epoch_loss=1.106425\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:53 INFO 139712694728512] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=1.10642492771\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:55 INFO 139712694728512] Epoch[55] Batch[5] avg_epoch_loss=1.268793\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=1.26879270871\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:55 INFO 139712694728512] Epoch[55] Batch [5]#011Speed: 180.31 samples/sec#011loss=1.268793\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:56 INFO 139712694728512] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4018.983840942383, \"sum\": 4018.983840942383, \"min\": 4018.983840942383}}, \"EndTime\": 1597163456.819723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163452.800269}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:56 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.267573687 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:56 INFO 139712694728512] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=55, train loss <loss>=1.22875013947\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:56 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:56 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_99021938-95fa-4d13-92e8-e1611339c377-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 135.98203659057617, \"sum\": 135.98203659057617, \"min\": 135.98203659057617}}, \"EndTime\": 1597163456.956413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163456.819815}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:57 INFO 139712694728512] Epoch[56] Batch[0] avg_epoch_loss=1.204980\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=1.20497953892\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:59 INFO 139712694728512] Epoch[56] Batch[5] avg_epoch_loss=1.299524\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=1.29952357213\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:30:59 INFO 139712694728512] Epoch[56] Batch [5]#011Speed: 182.19 samples/sec#011loss=1.299524\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:00 INFO 139712694728512] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3994.6727752685547, \"sum\": 3994.6727752685547, \"min\": 3994.6727752685547}}, \"EndTime\": 1597163460.95124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163456.956494}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:00 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=160.208066318 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:00 INFO 139712694728512] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=56, train loss <loss>=1.29489653111\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:00 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:01 INFO 139712694728512] Epoch[57] Batch[0] avg_epoch_loss=1.341421\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=1.34142076969\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:03 INFO 139712694728512] Epoch[57] Batch[5] avg_epoch_loss=1.285565\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=1.28556549549\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:03 INFO 139712694728512] Epoch[57] Batch [5]#011Speed: 178.72 samples/sec#011loss=1.285565\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:05 INFO 139712694728512] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4086.5190029144287, \"sum\": 4086.5190029144287, \"min\": 4086.5190029144287}}, \"EndTime\": 1597163465.038349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163460.95133}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:05 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.489970134 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:05 INFO 139712694728512] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=57, train loss <loss>=1.30081298351\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:05 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:05 INFO 139712694728512] Epoch[58] Batch[0] avg_epoch_loss=1.321017\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=1.32101714611\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:07 INFO 139712694728512] Epoch[58] Batch[5] avg_epoch_loss=1.282396\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=1.28239641587\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:07 INFO 139712694728512] Epoch[58] Batch [5]#011Speed: 180.79 samples/sec#011loss=1.282396\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] Epoch[58] Batch[10] avg_epoch_loss=1.270159\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=1.25547301769\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] Epoch[58] Batch [10]#011Speed: 179.93 samples/sec#011loss=1.255473\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4418.462038040161, \"sum\": 4418.462038040161, \"min\": 4418.462038040161}}, \"EndTime\": 1597163469.457382, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163465.038441}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.332168522 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=58, train loss <loss>=1.27015850761\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:09 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:10 INFO 139712694728512] Epoch[59] Batch[0] avg_epoch_loss=1.367780\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=1.36778008938\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:12 INFO 139712694728512] Epoch[59] Batch[5] avg_epoch_loss=1.343926\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=1.34392623107\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:12 INFO 139712694728512] Epoch[59] Batch [5]#011Speed: 179.53 samples/sec#011loss=1.343926\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:13 INFO 139712694728512] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4016.770839691162, \"sum\": 4016.770839691162, \"min\": 4016.770839691162}}, \"EndTime\": 1597163473.474677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163469.457469}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:13 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.83292763 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:13 INFO 139712694728512] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=59, train loss <loss>=1.29621552229\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:13 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:14 INFO 139712694728512] Epoch[60] Batch[0] avg_epoch_loss=1.271792\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=1.27179169655\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:16 INFO 139712694728512] Epoch[60] Batch[5] avg_epoch_loss=1.290940\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=1.29094018539\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:16 INFO 139712694728512] Epoch[60] Batch [5]#011Speed: 182.55 samples/sec#011loss=1.290940\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:17 INFO 139712694728512] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3999.476909637451, \"sum\": 3999.476909637451, \"min\": 3999.476909637451}}, \"EndTime\": 1597163477.474714, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163473.474772}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:17 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.766024931 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:17 INFO 139712694728512] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=60, train loss <loss>=1.30617083311\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:17 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:18 INFO 139712694728512] Epoch[61] Batch[0] avg_epoch_loss=1.394476\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=1.39447557926\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:20 INFO 139712694728512] Epoch[61] Batch[5] avg_epoch_loss=1.232654\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=1.23265405496\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:20 INFO 139712694728512] Epoch[61] Batch [5]#011Speed: 183.09 samples/sec#011loss=1.232654\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] Epoch[61] Batch[10] avg_epoch_loss=1.151326\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=1.05373292267\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] Epoch[61] Batch [10]#011Speed: 182.67 samples/sec#011loss=1.053733\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4304.563999176025, \"sum\": 4304.563999176025, \"min\": 4304.563999176025}}, \"EndTime\": 1597163481.779819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163477.474797}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.068915647 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=61, train loss <loss>=1.15132626756\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:21 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_037a5098-615f-4685-95f8-e400c4a80769-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.0450611114502, \"sum\": 125.0450611114502, \"min\": 125.0450611114502}}, \"EndTime\": 1597163481.90548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163481.779906}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:22 INFO 139712694728512] Epoch[62] Batch[0] avg_epoch_loss=1.538570\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=1.53857004642\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:24 INFO 139712694728512] Epoch[62] Batch[5] avg_epoch_loss=1.447206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=1.44720582167\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:24 INFO 139712694728512] Epoch[62] Batch [5]#011Speed: 182.91 samples/sec#011loss=1.447206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:25 INFO 139712694728512] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3972.5308418273926, \"sum\": 3972.5308418273926, \"min\": 3972.5308418273926}}, \"EndTime\": 1597163485.878154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163481.905553}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:25 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.080249815 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:25 INFO 139712694728512] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=62, train loss <loss>=1.46461036205\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:25 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:26 INFO 139712694728512] Epoch[63] Batch[0] avg_epoch_loss=1.367825\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=1.36782467365\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:28 INFO 139712694728512] Epoch[63] Batch[5] avg_epoch_loss=1.324417\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=1.32441685597\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:28 INFO 139712694728512] Epoch[63] Batch [5]#011Speed: 182.28 samples/sec#011loss=1.324417\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:29 INFO 139712694728512] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3985.239028930664, \"sum\": 3985.239028930664, \"min\": 3985.239028930664}}, \"EndTime\": 1597163489.86398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163485.878248}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:29 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=160.336472051 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:29 INFO 139712694728512] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=63, train loss <loss>=1.3046667695\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:29 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:30 INFO 139712694728512] Epoch[64] Batch[0] avg_epoch_loss=1.337985\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:30 INFO 139712694728512] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=1.33798539639\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:32 INFO 139712694728512] Epoch[64] Batch[5] avg_epoch_loss=1.315575\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=1.31557484468\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:32 INFO 139712694728512] Epoch[64] Batch [5]#011Speed: 180.53 samples/sec#011loss=1.315575\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:33 INFO 139712694728512] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4039.5500659942627, \"sum\": 4039.5500659942627, \"min\": 4039.5500659942627}}, \"EndTime\": 1597163493.904075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163489.864071}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:33 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.180724482 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:33 INFO 139712694728512] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=64, train loss <loss>=1.29064188004\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:33 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:34 INFO 139712694728512] Epoch[65] Batch[0] avg_epoch_loss=1.262853\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:34 INFO 139712694728512] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=1.26285290718\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:36 INFO 139712694728512] Epoch[65] Batch[5] avg_epoch_loss=1.259388\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=1.25938779116\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:36 INFO 139712694728512] Epoch[65] Batch [5]#011Speed: 181.24 samples/sec#011loss=1.259388\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] Epoch[65] Batch[10] avg_epoch_loss=1.225546\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=1.18493629694\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] Epoch[65] Batch [10]#011Speed: 179.85 samples/sec#011loss=1.184936\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4413.931131362915, \"sum\": 4413.931131362915, \"min\": 4413.931131362915}}, \"EndTime\": 1597163498.318583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163493.904168}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.483337196 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=65, train loss <loss>=1.22554620288\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:38 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:39 INFO 139712694728512] Epoch[66] Batch[0] avg_epoch_loss=1.489364\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=1.48936414719\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:40 INFO 139712694728512] Epoch[66] Batch[5] avg_epoch_loss=1.199244\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=1.19924406211\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:40 INFO 139712694728512] Epoch[66] Batch [5]#011Speed: 180.04 samples/sec#011loss=1.199244\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:42 INFO 139712694728512] Epoch[66] Batch[10] avg_epoch_loss=1.225120\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=1.25617105961\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:42 INFO 139712694728512] Epoch[66] Batch [10]#011Speed: 178.61 samples/sec#011loss=1.256171\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:43 INFO 139712694728512] processed a total of 706 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4837.048053741455, \"sum\": 4837.048053741455, \"min\": 4837.048053741455}}, \"EndTime\": 1597163503.156191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163498.31867}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:43 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=145.952745511 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:43 INFO 139712694728512] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=66, train loss <loss>=1.30208336314\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:43 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:43 INFO 139712694728512] Epoch[67] Batch[0] avg_epoch_loss=1.139314\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=1.13931429386\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:45 INFO 139712694728512] Epoch[67] Batch[5] avg_epoch_loss=1.246722\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=1.24672158559\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:45 INFO 139712694728512] Epoch[67] Batch [5]#011Speed: 183.78 samples/sec#011loss=1.246722\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:47 INFO 139712694728512] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4019.304037094116, \"sum\": 4019.304037094116, \"min\": 4019.304037094116}}, \"EndTime\": 1597163507.176114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163503.156283}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:47 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.728741704 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:47 INFO 139712694728512] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:47 INFO 139712694728512] #quality_metric: host=algo-1, epoch=67, train loss <loss>=1.29459139109\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:47 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:48 INFO 139712694728512] Epoch[68] Batch[0] avg_epoch_loss=1.311506\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=1.31150591373\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:49 INFO 139712694728512] Epoch[68] Batch[5] avg_epoch_loss=1.239005\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=1.23900477091\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:49 INFO 139712694728512] Epoch[68] Batch [5]#011Speed: 181.63 samples/sec#011loss=1.239005\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] Epoch[68] Batch[10] avg_epoch_loss=1.189471\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=1.13003064394\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] Epoch[68] Batch [10]#011Speed: 178.71 samples/sec#011loss=1.130031\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4397.356986999512, \"sum\": 4397.356986999512, \"min\": 4397.356986999512}}, \"EndTime\": 1597163511.574023, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163507.176207}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.948846207 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=68, train loss <loss>=1.18947107684\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:51 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:52 INFO 139712694728512] Epoch[69] Batch[0] avg_epoch_loss=1.347351\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=1.34735059738\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:54 INFO 139712694728512] Epoch[69] Batch[5] avg_epoch_loss=1.254179\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=1.25417862336\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:54 INFO 139712694728512] Epoch[69] Batch [5]#011Speed: 177.02 samples/sec#011loss=1.254179\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:55 INFO 139712694728512] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4077.932834625244, \"sum\": 4077.932834625244, \"min\": 4077.932834625244}}, \"EndTime\": 1597163515.652508, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163511.574112}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:55 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.239703024 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:55 INFO 139712694728512] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=69, train loss <loss>=1.21695723534\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:55 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:56 INFO 139712694728512] Epoch[70] Batch[0] avg_epoch_loss=1.257274\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=1.25727427006\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:58 INFO 139712694728512] Epoch[70] Batch[5] avg_epoch_loss=1.239963\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=1.2399627169\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:31:58 INFO 139712694728512] Epoch[70] Batch [5]#011Speed: 182.16 samples/sec#011loss=1.239963\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] Epoch[70] Batch[10] avg_epoch_loss=1.174718\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=1.09642351866\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] Epoch[70] Batch [10]#011Speed: 182.61 samples/sec#011loss=1.096424\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4368.600845336914, \"sum\": 4368.600845336914, \"min\": 4368.600845336914}}, \"EndTime\": 1597163520.02172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163515.652602}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.700392687 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=70, train loss <loss>=1.17471762679\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] Epoch[71] Batch[0] avg_epoch_loss=1.310241\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=1.31024122238\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:02 INFO 139712694728512] Epoch[71] Batch[5] avg_epoch_loss=1.243970\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=1.243970414\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:02 INFO 139712694728512] Epoch[71] Batch [5]#011Speed: 177.20 samples/sec#011loss=1.243970\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:04 INFO 139712694728512] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4027.6529788970947, \"sum\": 4027.6529788970947, \"min\": 4027.6529788970947}}, \"EndTime\": 1597163524.049894, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163520.021806}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:04 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.442348847 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:04 INFO 139712694728512] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=71, train loss <loss>=1.39203048944\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:04 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:04 INFO 139712694728512] Epoch[72] Batch[0] avg_epoch_loss=1.155490\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=1.15548968315\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:06 INFO 139712694728512] Epoch[72] Batch[5] avg_epoch_loss=1.228978\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=1.22897835573\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:06 INFO 139712694728512] Epoch[72] Batch [5]#011Speed: 180.42 samples/sec#011loss=1.228978\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:08 INFO 139712694728512] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3994.4489002227783, \"sum\": 3994.4489002227783, \"min\": 3994.4489002227783}}, \"EndTime\": 1597163528.044901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163524.049962}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:08 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.961680139 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:08 INFO 139712694728512] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=72, train loss <loss>=1.23431161642\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:08 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:08 INFO 139712694728512] Epoch[73] Batch[0] avg_epoch_loss=1.155568\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=1.15556848049\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:10 INFO 139712694728512] Epoch[73] Batch[5] avg_epoch_loss=1.234401\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=1.23440092802\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:10 INFO 139712694728512] Epoch[73] Batch [5]#011Speed: 181.39 samples/sec#011loss=1.234401\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:12 INFO 139712694728512] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3998.508930206299, \"sum\": 3998.508930206299, \"min\": 3998.508930206299}}, \"EndTime\": 1597163532.044013, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163528.044981}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:12 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.052567661 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:12 INFO 139712694728512] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=73, train loss <loss>=1.25193761587\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:12 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:12 INFO 139712694728512] Epoch[74] Batch[0] avg_epoch_loss=1.487060\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=1.48705983162\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:14 INFO 139712694728512] Epoch[74] Batch[5] avg_epoch_loss=1.266552\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=1.26655157407\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:14 INFO 139712694728512] Epoch[74] Batch [5]#011Speed: 177.48 samples/sec#011loss=1.266552\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] Epoch[74] Batch[10] avg_epoch_loss=1.240918\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=1.21015734673\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] Epoch[74] Batch [10]#011Speed: 180.45 samples/sec#011loss=1.210157\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4408.720016479492, \"sum\": 4408.720016479492, \"min\": 4408.720016479492}}, \"EndTime\": 1597163536.453304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163532.044105}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.111491341 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=74, train loss <loss>=1.24091783437\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:16 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:17 INFO 139712694728512] Epoch[75] Batch[0] avg_epoch_loss=1.213873\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=1.21387326717\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:19 INFO 139712694728512] Epoch[75] Batch[5] avg_epoch_loss=1.271052\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=1.27105204264\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:19 INFO 139712694728512] Epoch[75] Batch [5]#011Speed: 180.03 samples/sec#011loss=1.271052\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:20 INFO 139712694728512] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4053.7240505218506, \"sum\": 4053.7240505218506, \"min\": 4053.7240505218506}}, \"EndTime\": 1597163540.507564, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163536.453388}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:20 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.381757187 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:20 INFO 139712694728512] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=75, train loss <loss>=1.26735862494\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:20 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:21 INFO 139712694728512] Epoch[76] Batch[0] avg_epoch_loss=1.359260\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=1.35926008224\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:23 INFO 139712694728512] Epoch[76] Batch[5] avg_epoch_loss=1.312678\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=1.31267817815\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:23 INFO 139712694728512] Epoch[76] Batch [5]#011Speed: 176.76 samples/sec#011loss=1.312678\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:24 INFO 139712694728512] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4040.7369136810303, \"sum\": 4040.7369136810303, \"min\": 4040.7369136810303}}, \"EndTime\": 1597163544.548904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163540.507636}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:24 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.453098532 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:24 INFO 139712694728512] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=76, train loss <loss>=1.27603024244\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:24 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:25 INFO 139712694728512] Epoch[77] Batch[0] avg_epoch_loss=1.359246\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=1.35924601555\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:27 INFO 139712694728512] Epoch[77] Batch[5] avg_epoch_loss=1.264028\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=1.26402805249\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:27 INFO 139712694728512] Epoch[77] Batch [5]#011Speed: 178.23 samples/sec#011loss=1.264028\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] Epoch[77] Batch[10] avg_epoch_loss=1.209424\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=1.14389965534\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] Epoch[77] Batch [10]#011Speed: 179.79 samples/sec#011loss=1.143900\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4396.0840702056885, \"sum\": 4396.0840702056885, \"min\": 4396.0840702056885}}, \"EndTime\": 1597163548.945602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163544.548987}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.541697139 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=77, train loss <loss>=1.2094242356\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:28 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:29 INFO 139712694728512] Epoch[78] Batch[0] avg_epoch_loss=1.136605\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=1.1366045475\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:31 INFO 139712694728512] Epoch[78] Batch[5] avg_epoch_loss=1.236091\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=1.23609137535\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:31 INFO 139712694728512] Epoch[78] Batch [5]#011Speed: 180.54 samples/sec#011loss=1.236091\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] Epoch[78] Batch[10] avg_epoch_loss=1.242456\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=1.25009304285\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] Epoch[78] Batch [10]#011Speed: 177.78 samples/sec#011loss=1.250093\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4403.11598777771, \"sum\": 4403.11598777771, \"min\": 4403.11598777771}}, \"EndTime\": 1597163553.349273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163548.945676}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.205116623 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=78, train loss <loss>=1.24245576967\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:33 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:34 INFO 139712694728512] Epoch[79] Batch[0] avg_epoch_loss=1.101098\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:34 INFO 139712694728512] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=1.10109758377\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:35 INFO 139712694728512] Epoch[79] Batch[5] avg_epoch_loss=1.232627\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=1.2326267163\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:35 INFO 139712694728512] Epoch[79] Batch [5]#011Speed: 179.41 samples/sec#011loss=1.232627\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:37 INFO 139712694728512] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4051.0618686676025, \"sum\": 4051.0618686676025, \"min\": 4051.0618686676025}}, \"EndTime\": 1597163557.40092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163553.349341}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:37 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.795478014 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:37 INFO 139712694728512] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=79, train loss <loss>=1.25073444843\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:37 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:38 INFO 139712694728512] Epoch[80] Batch[0] avg_epoch_loss=1.390690\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=1.39069020748\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:39 INFO 139712694728512] Epoch[80] Batch[5] avg_epoch_loss=1.264662\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=1.26466210683\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:39 INFO 139712694728512] Epoch[80] Batch [5]#011Speed: 181.03 samples/sec#011loss=1.264662\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] Epoch[80] Batch[10] avg_epoch_loss=1.223058\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=1.17313241959\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] Epoch[80] Batch [10]#011Speed: 178.92 samples/sec#011loss=1.173132\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4376.7900466918945, \"sum\": 4376.7900466918945, \"min\": 4376.7900466918945}}, \"EndTime\": 1597163561.778259, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163557.400991}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.136262613 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=80, train loss <loss>=1.22305770354\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:41 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:42 INFO 139712694728512] Epoch[81] Batch[0] avg_epoch_loss=1.134195\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=1.13419544697\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:44 INFO 139712694728512] Epoch[81] Batch[5] avg_epoch_loss=1.183442\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=1.18344173829\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:44 INFO 139712694728512] Epoch[81] Batch [5]#011Speed: 174.67 samples/sec#011loss=1.183442\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:45 INFO 139712694728512] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4087.818145751953, \"sum\": 4087.818145751953, \"min\": 4087.818145751953}}, \"EndTime\": 1597163565.866787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163561.778328}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:45 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.463554608 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:45 INFO 139712694728512] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=81, train loss <loss>=1.20853847265\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:45 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:46 INFO 139712694728512] Epoch[82] Batch[0] avg_epoch_loss=1.351316\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=1.35131621361\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:48 INFO 139712694728512] Epoch[82] Batch[5] avg_epoch_loss=1.218454\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=1.21845394373\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:48 INFO 139712694728512] Epoch[82] Batch [5]#011Speed: 180.07 samples/sec#011loss=1.218454\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] Epoch[82] Batch[10] avg_epoch_loss=1.220680\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=1.22335059643\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] Epoch[82] Batch [10]#011Speed: 180.61 samples/sec#011loss=1.223351\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4390.958786010742, \"sum\": 4390.958786010742, \"min\": 4390.958786010742}}, \"EndTime\": 1597163570.258334, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163565.86688}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.760074379 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=82, train loss <loss>=1.22067969496\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:50 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:51 INFO 139712694728512] Epoch[83] Batch[0] avg_epoch_loss=1.268205\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=1.26820492744\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:52 INFO 139712694728512] Epoch[83] Batch[5] avg_epoch_loss=1.224154\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=1.2241538167\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:52 INFO 139712694728512] Epoch[83] Batch [5]#011Speed: 176.49 samples/sec#011loss=1.224154\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:54 INFO 139712694728512] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4091.4859771728516, \"sum\": 4091.4859771728516, \"min\": 4091.4859771728516}}, \"EndTime\": 1597163574.350378, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163570.258421}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:54 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.062777489 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:54 INFO 139712694728512] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=83, train loss <loss>=1.15060704947\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:54 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:54 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_d04c181b-cb16-4ae6-b7cb-9a75f67466af-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 117.1579360961914, \"sum\": 117.1579360961914, \"min\": 117.1579360961914}}, \"EndTime\": 1597163574.468243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163574.350473}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:55 INFO 139712694728512] Epoch[84] Batch[0] avg_epoch_loss=1.092854\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=1.09285378456\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:57 INFO 139712694728512] Epoch[84] Batch[5] avg_epoch_loss=1.112522\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=1.11252238353\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:57 INFO 139712694728512] Epoch[84] Batch [5]#011Speed: 180.29 samples/sec#011loss=1.112522\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:58 INFO 139712694728512] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4011.399030685425, \"sum\": 4011.399030685425, \"min\": 4011.399030685425}}, \"EndTime\": 1597163578.479801, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163574.468328}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:58 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.545792791 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:58 INFO 139712694728512] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=84, train loss <loss>=1.16565487385\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:58 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:59 INFO 139712694728512] Epoch[85] Batch[0] avg_epoch_loss=1.229515\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:32:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=1.22951507568\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:01 INFO 139712694728512] Epoch[85] Batch[5] avg_epoch_loss=1.227592\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=1.22759205103\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:01 INFO 139712694728512] Epoch[85] Batch [5]#011Speed: 181.29 samples/sec#011loss=1.227592\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:02 INFO 139712694728512] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4078.040838241577, \"sum\": 4078.040838241577, \"min\": 4078.040838241577}}, \"EndTime\": 1597163582.558377, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163578.479892}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:02 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=145.898600447 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:02 INFO 139712694728512] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=85, train loss <loss>=1.29972848892\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:02 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:03 INFO 139712694728512] Epoch[86] Batch[0] avg_epoch_loss=1.013317\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=1.01331686974\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:05 INFO 139712694728512] Epoch[86] Batch[5] avg_epoch_loss=1.231509\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=1.23150946697\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:05 INFO 139712694728512] Epoch[86] Batch [5]#011Speed: 181.64 samples/sec#011loss=1.231509\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] Epoch[86] Batch[10] avg_epoch_loss=1.208004\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=1.17979738712\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] Epoch[86] Batch [10]#011Speed: 180.81 samples/sec#011loss=1.179797\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4389.374017715454, \"sum\": 4389.374017715454, \"min\": 4389.374017715454}}, \"EndTime\": 1597163586.948348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163582.55847}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.903319776 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=86, train loss <loss>=1.20800397613\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:06 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:07 INFO 139712694728512] Epoch[87] Batch[0] avg_epoch_loss=1.148065\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=1.14806520939\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:09 INFO 139712694728512] Epoch[87] Batch[5] avg_epoch_loss=1.176127\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=1.17612659931\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:09 INFO 139712694728512] Epoch[87] Batch [5]#011Speed: 181.12 samples/sec#011loss=1.176127\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4066.585063934326, \"sum\": 4066.585063934326, \"min\": 4066.585063934326}}, \"EndTime\": 1597163591.015451, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163586.948432}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.439818661 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=87, train loss <loss>=1.14233540893\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_2b3f6db4-a9c7-4d9f-b757-2e50998dd2df-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 131.80994987487793, \"sum\": 131.80994987487793, \"min\": 131.80994987487793}}, \"EndTime\": 1597163591.147902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163591.015567}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] Epoch[88] Batch[0] avg_epoch_loss=1.180184\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=1.1801841259\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:13 INFO 139712694728512] Epoch[88] Batch[5] avg_epoch_loss=1.195416\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=1.1954159538\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:13 INFO 139712694728512] Epoch[88] Batch [5]#011Speed: 174.90 samples/sec#011loss=1.195416\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] Epoch[88] Batch[10] avg_epoch_loss=1.142486\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=1.07897022367\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] Epoch[88] Batch [10]#011Speed: 182.75 samples/sec#011loss=1.078970\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4420.817852020264, \"sum\": 4420.817852020264, \"min\": 4420.817852020264}}, \"EndTime\": 1597163595.568882, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163591.147994}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.646283356 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=88, train loss <loss>=1.14248607646\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:15 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:16 INFO 139712694728512] Epoch[89] Batch[0] avg_epoch_loss=0.995017\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.995016872883\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:18 INFO 139712694728512] Epoch[89] Batch[5] avg_epoch_loss=1.123676\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=1.12367551525\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:18 INFO 139712694728512] Epoch[89] Batch [5]#011Speed: 180.51 samples/sec#011loss=1.123676\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:19 INFO 139712694728512] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4035.336971282959, \"sum\": 4035.336971282959, \"min\": 4035.336971282959}}, \"EndTime\": 1597163599.60474, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163595.568978}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:19 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.665215159 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:19 INFO 139712694728512] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=89, train loss <loss>=1.11707951427\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:19 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:19 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_693909a9-4adc-40b5-8831-a69dfea994d8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 116.42003059387207, \"sum\": 116.42003059387207, \"min\": 116.42003059387207}}, \"EndTime\": 1597163599.721736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163599.604808}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:20 INFO 139712694728512] Epoch[90] Batch[0] avg_epoch_loss=1.122112\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=1.12211215496\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:22 INFO 139712694728512] Epoch[90] Batch[5] avg_epoch_loss=1.211640\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=1.21164025863\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:22 INFO 139712694728512] Epoch[90] Batch [5]#011Speed: 179.06 samples/sec#011loss=1.211640\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] Epoch[90] Batch[10] avg_epoch_loss=1.126420\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=1.0241558671\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] Epoch[90] Batch [10]#011Speed: 180.32 samples/sec#011loss=1.024156\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4373.450994491577, \"sum\": 4373.450994491577, \"min\": 4373.450994491577}}, \"EndTime\": 1597163604.095331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163599.721814}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.220907111 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=90, train loss <loss>=1.12642008066\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] Epoch[91] Batch[0] avg_epoch_loss=1.285178\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=1.2851780653\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:26 INFO 139712694728512] Epoch[91] Batch[5] avg_epoch_loss=1.165846\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=1.16584612926\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:26 INFO 139712694728512] Epoch[91] Batch [5]#011Speed: 182.49 samples/sec#011loss=1.165846\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] Epoch[91] Batch[10] avg_epoch_loss=1.204737\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=1.25140562057\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] Epoch[91] Batch [10]#011Speed: 178.70 samples/sec#011loss=1.251406\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4368.237018585205, \"sum\": 4368.237018585205, \"min\": 4368.237018585205}}, \"EndTime\": 1597163608.464145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163604.095403}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.640951645 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=91, train loss <loss>=1.20473680713\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:28 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:29 INFO 139712694728512] Epoch[92] Batch[0] avg_epoch_loss=1.249594\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=1.24959397316\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:31 INFO 139712694728512] Epoch[92] Batch[5] avg_epoch_loss=1.205987\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=1.20598733425\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:31 INFO 139712694728512] Epoch[92] Batch [5]#011Speed: 181.93 samples/sec#011loss=1.205987\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] Epoch[92] Batch[10] avg_epoch_loss=1.191465\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=1.17403857708\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] Epoch[92] Batch [10]#011Speed: 179.37 samples/sec#011loss=1.174039\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4354.3760776519775, \"sum\": 4354.3760776519775, \"min\": 4354.3760776519775}}, \"EndTime\": 1597163612.819079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163608.46423}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.944456718 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=92, train loss <loss>=1.1914651719\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:32 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:33 INFO 139712694728512] Epoch[93] Batch[0] avg_epoch_loss=1.079341\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=1.07934069633\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:35 INFO 139712694728512] Epoch[93] Batch[5] avg_epoch_loss=1.085704\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=1.08570382992\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:35 INFO 139712694728512] Epoch[93] Batch [5]#011Speed: 182.67 samples/sec#011loss=1.085704\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] Epoch[93] Batch[10] avg_epoch_loss=1.156463\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=1.24137452841\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] Epoch[93] Batch [10]#011Speed: 181.19 samples/sec#011loss=1.241375\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4336.277961730957, \"sum\": 4336.277961730957, \"min\": 4336.277961730957}}, \"EndTime\": 1597163617.155964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163612.819182}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.889239806 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=93, train loss <loss>=1.15646323833\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] Epoch[94] Batch[0] avg_epoch_loss=1.158257\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=1.15825653076\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:39 INFO 139712694728512] Epoch[94] Batch[5] avg_epoch_loss=1.119580\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=1.11958018939\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:39 INFO 139712694728512] Epoch[94] Batch [5]#011Speed: 182.21 samples/sec#011loss=1.119580\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:41 INFO 139712694728512] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3998.584985733032, \"sum\": 3998.584985733032, \"min\": 3998.584985733032}}, \"EndTime\": 1597163621.155126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163617.156056}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:41 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.301089146 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:41 INFO 139712694728512] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=94, train loss <loss>=1.14235021472\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:41 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:41 INFO 139712694728512] Epoch[95] Batch[0] avg_epoch_loss=1.172037\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=1.17203700542\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:43 INFO 139712694728512] Epoch[95] Batch[5] avg_epoch_loss=1.165509\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=1.16550890605\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:43 INFO 139712694728512] Epoch[95] Batch [5]#011Speed: 180.42 samples/sec#011loss=1.165509\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] Epoch[95] Batch[10] avg_epoch_loss=1.123449\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=1.07297800779\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] Epoch[95] Batch [10]#011Speed: 179.76 samples/sec#011loss=1.072978\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4363.4819984436035, \"sum\": 4363.4819984436035, \"min\": 4363.4819984436035}}, \"EndTime\": 1597163625.519306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163621.155201}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.126096906 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=95, train loss <loss>=1.12344940684\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:45 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:46 INFO 139712694728512] Epoch[96] Batch[0] avg_epoch_loss=1.409104\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=1.4091039896\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:48 INFO 139712694728512] Epoch[96] Batch[5] avg_epoch_loss=1.234824\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=1.23482350508\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:48 INFO 139712694728512] Epoch[96] Batch [5]#011Speed: 178.63 samples/sec#011loss=1.234824\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] Epoch[96] Batch[10] avg_epoch_loss=1.347625\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=1.48298683167\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] Epoch[96] Batch [10]#011Speed: 181.45 samples/sec#011loss=1.482987\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4361.724853515625, \"sum\": 4361.724853515625, \"min\": 4361.724853515625}}, \"EndTime\": 1597163629.881618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163625.519387}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.560743209 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=96, train loss <loss>=1.34762501717\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:49 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:50 INFO 139712694728512] Epoch[97] Batch[0] avg_epoch_loss=1.076655\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=1.07665514946\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:52 INFO 139712694728512] Epoch[97] Batch[5] avg_epoch_loss=1.103498\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=1.103497684\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:52 INFO 139712694728512] Epoch[97] Batch [5]#011Speed: 178.85 samples/sec#011loss=1.103498\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] Epoch[97] Batch[10] avg_epoch_loss=1.151219\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=1.20848546028\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] Epoch[97] Batch [10]#011Speed: 182.51 samples/sec#011loss=1.208485\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4356.019973754883, \"sum\": 4356.019973754883, \"min\": 4356.019973754883}}, \"EndTime\": 1597163634.238178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163629.881704}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.362406316 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=97, train loss <loss>=1.15121940049\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:54 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:55 INFO 139712694728512] Epoch[98] Batch[0] avg_epoch_loss=1.175889\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=1.17588925362\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:56 INFO 139712694728512] Epoch[98] Batch[5] avg_epoch_loss=1.196303\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=1.19630308946\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:56 INFO 139712694728512] Epoch[98] Batch [5]#011Speed: 179.87 samples/sec#011loss=1.196303\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] Epoch[98] Batch[10] avg_epoch_loss=1.250927\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=1.3164760828\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] Epoch[98] Batch [10]#011Speed: 179.29 samples/sec#011loss=1.316476\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4391.464948654175, \"sum\": 4391.464948654175, \"min\": 4391.464948654175}}, \"EndTime\": 1597163638.63015, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163634.238263}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.466093701 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=98, train loss <loss>=1.25092717734\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:58 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:59 INFO 139712694728512] Epoch[99] Batch[0] avg_epoch_loss=1.072037\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:33:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=1.07203662395\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:01 INFO 139712694728512] Epoch[99] Batch[5] avg_epoch_loss=1.130040\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=1.13003991048\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:01 INFO 139712694728512] Epoch[99] Batch [5]#011Speed: 182.40 samples/sec#011loss=1.130040\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] Epoch[99] Batch[10] avg_epoch_loss=1.120126\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=1.10822999477\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] Epoch[99] Batch [10]#011Speed: 178.60 samples/sec#011loss=1.108230\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4365.533828735352, \"sum\": 4365.533828735352, \"min\": 4365.533828735352}}, \"EndTime\": 1597163642.996224, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163638.630223}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.448359054 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=99, train loss <loss>=1.12012631243\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:02 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:03 INFO 139712694728512] Epoch[100] Batch[0] avg_epoch_loss=1.007982\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=1.00798153877\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:05 INFO 139712694728512] Epoch[100] Batch[5] avg_epoch_loss=1.165073\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=1.16507273912\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:05 INFO 139712694728512] Epoch[100] Batch [5]#011Speed: 181.47 samples/sec#011loss=1.165073\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:07 INFO 139712694728512] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4033.1990718841553, \"sum\": 4033.1990718841553, \"min\": 4033.1990718841553}}, \"EndTime\": 1597163647.029953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163642.996311}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:07 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.008239213 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:07 INFO 139712694728512] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=100, train loss <loss>=1.18254796267\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:07 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:07 INFO 139712694728512] Epoch[101] Batch[0] avg_epoch_loss=1.263160\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=1.26315951347\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:09 INFO 139712694728512] Epoch[101] Batch[5] avg_epoch_loss=1.134065\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=1.13406523069\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:09 INFO 139712694728512] Epoch[101] Batch [5]#011Speed: 179.68 samples/sec#011loss=1.134065\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] Epoch[101] Batch[10] avg_epoch_loss=1.083868\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=1.02363029718\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] Epoch[101] Batch [10]#011Speed: 176.60 samples/sec#011loss=1.023630\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4457.751989364624, \"sum\": 4457.751989364624, \"min\": 4457.751989364624}}, \"EndTime\": 1597163651.488275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163647.030047}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.744468606 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=101, train loss <loss>=1.08386753364\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:11 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_0fd9d575-0ab0-41f3-852e-6bfeb012c7b5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.99897384643555, \"sum\": 125.99897384643555, \"min\": 125.99897384643555}}, \"EndTime\": 1597163651.614889, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163651.488361}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:12 INFO 139712694728512] Epoch[102] Batch[0] avg_epoch_loss=1.281875\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=1.28187513351\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:14 INFO 139712694728512] Epoch[102] Batch[5] avg_epoch_loss=1.169475\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=1.16947548588\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:14 INFO 139712694728512] Epoch[102] Batch [5]#011Speed: 177.44 samples/sec#011loss=1.169475\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:15 INFO 139712694728512] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4052.2191524505615, \"sum\": 4052.2191524505615, \"min\": 4052.2191524505615}}, \"EndTime\": 1597163655.667269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163651.614981}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:15 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.542671451 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:15 INFO 139712694728512] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=102, train loss <loss>=1.20388490558\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:15 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:16 INFO 139712694728512] Epoch[103] Batch[0] avg_epoch_loss=1.149378\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=1.14937829971\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:18 INFO 139712694728512] Epoch[103] Batch[5] avg_epoch_loss=1.174457\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=1.17445685466\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:18 INFO 139712694728512] Epoch[103] Batch [5]#011Speed: 177.44 samples/sec#011loss=1.174457\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] Epoch[103] Batch[10] avg_epoch_loss=1.116768\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=1.04754190445\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] Epoch[103] Batch [10]#011Speed: 180.24 samples/sec#011loss=1.047542\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4443.861961364746, \"sum\": 4443.861961364746, \"min\": 4443.861961364746}}, \"EndTime\": 1597163660.111717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163655.667364}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.165182108 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=103, train loss <loss>=1.11676824093\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] Epoch[104] Batch[0] avg_epoch_loss=1.130630\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=1.13063013554\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:22 INFO 139712694728512] Epoch[104] Batch[5] avg_epoch_loss=1.204775\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=1.20477495591\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:22 INFO 139712694728512] Epoch[104] Batch [5]#011Speed: 178.37 samples/sec#011loss=1.204775\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] Epoch[104] Batch[10] avg_epoch_loss=1.258017\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=1.32190759182\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] Epoch[104] Batch [10]#011Speed: 178.35 samples/sec#011loss=1.321908\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4432.780027389526, \"sum\": 4432.780027389526, \"min\": 4432.780027389526}}, \"EndTime\": 1597163664.545022, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163660.111804}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.593562616 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=104, train loss <loss>=1.25801706314\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:24 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:25 INFO 139712694728512] Epoch[105] Batch[0] avg_epoch_loss=1.174356\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=1.17435574532\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:27 INFO 139712694728512] Epoch[105] Batch[5] avg_epoch_loss=1.180031\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=1.18003121018\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:27 INFO 139712694728512] Epoch[105] Batch [5]#011Speed: 183.99 samples/sec#011loss=1.180031\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:28 INFO 139712694728512] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3988.968849182129, \"sum\": 3988.968849182129, \"min\": 3988.968849182129}}, \"EndTime\": 1597163668.534537, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163664.54511}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:28 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.161808588 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:28 INFO 139712694728512] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=105, train loss <loss>=1.20906805396\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:28 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:29 INFO 139712694728512] Epoch[106] Batch[0] avg_epoch_loss=1.114180\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=1.11417973042\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:31 INFO 139712694728512] Epoch[106] Batch[5] avg_epoch_loss=1.191722\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=1.19172155857\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:31 INFO 139712694728512] Epoch[106] Batch [5]#011Speed: 181.54 samples/sec#011loss=1.191722\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:32 INFO 139712694728512] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4004.8599243164062, \"sum\": 4004.8599243164062, \"min\": 4004.8599243164062}}, \"EndTime\": 1597163672.53997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163668.534631}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:32 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.05765309 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:32 INFO 139712694728512] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=106, train loss <loss>=1.10258660913\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:32 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:33 INFO 139712694728512] Epoch[107] Batch[0] avg_epoch_loss=1.051586\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=1.05158603191\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:35 INFO 139712694728512] Epoch[107] Batch[5] avg_epoch_loss=1.141107\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=1.1411070923\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:35 INFO 139712694728512] Epoch[107] Batch [5]#011Speed: 180.96 samples/sec#011loss=1.141107\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] Epoch[107] Batch[10] avg_epoch_loss=1.027689\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=0.891587254405\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] Epoch[107] Batch [10]#011Speed: 181.14 samples/sec#011loss=0.891587\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4382.697105407715, \"sum\": 4382.697105407715, \"min\": 4382.697105407715}}, \"EndTime\": 1597163676.923236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163672.540064}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.252904619 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=107, train loss <loss>=1.02768898417\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:36 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:37 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_857ca2e6-8363-45f8-9eee-960628bbfd2b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 127.15911865234375, \"sum\": 127.15911865234375, \"min\": 127.15911865234375}}, \"EndTime\": 1597163677.051051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163676.923321}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:37 INFO 139712694728512] Epoch[108] Batch[0] avg_epoch_loss=1.138968\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=1.13896799088\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:39 INFO 139712694728512] Epoch[108] Batch[5] avg_epoch_loss=1.146818\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=1.14681829015\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:39 INFO 139712694728512] Epoch[108] Batch [5]#011Speed: 181.12 samples/sec#011loss=1.146818\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] Epoch[108] Batch[10] avg_epoch_loss=1.107442\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=1.06019032001\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] Epoch[108] Batch [10]#011Speed: 180.56 samples/sec#011loss=1.060190\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4403.03897857666, \"sum\": 4403.03897857666, \"min\": 4403.03897857666}}, \"EndTime\": 1597163681.454231, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163677.051127}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.48559944 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=108, train loss <loss>=1.10744194009\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:41 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:42 INFO 139712694728512] Epoch[109] Batch[0] avg_epoch_loss=0.898447\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=0.898446559906\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:44 INFO 139712694728512] Epoch[109] Batch[5] avg_epoch_loss=1.178315\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=1.17831544081\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:44 INFO 139712694728512] Epoch[109] Batch [5]#011Speed: 177.65 samples/sec#011loss=1.178315\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:45 INFO 139712694728512] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4086.5399837493896, \"sum\": 4086.5399837493896, \"min\": 4086.5399837493896}}, \"EndTime\": 1597163685.541274, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163681.454316}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:45 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.467918358 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:45 INFO 139712694728512] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=109, train loss <loss>=1.15555121303\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:45 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:46 INFO 139712694728512] Epoch[110] Batch[0] avg_epoch_loss=1.044508\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=1.04450798035\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:48 INFO 139712694728512] Epoch[110] Batch[5] avg_epoch_loss=1.169751\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=1.16975140572\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:48 INFO 139712694728512] Epoch[110] Batch [5]#011Speed: 179.05 samples/sec#011loss=1.169751\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] Epoch[110] Batch[10] avg_epoch_loss=1.190166\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=1.2146625042\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] Epoch[110] Batch [10]#011Speed: 180.23 samples/sec#011loss=1.214663\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4426.167964935303, \"sum\": 4426.167964935303, \"min\": 4426.167964935303}}, \"EndTime\": 1597163689.968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163685.541368}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.338602868 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=110, train loss <loss>=1.19016554139\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:49 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:50 INFO 139712694728512] Epoch[111] Batch[0] avg_epoch_loss=1.078359\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=1.07835936546\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:52 INFO 139712694728512] Epoch[111] Batch[5] avg_epoch_loss=1.176956\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=1.17695564032\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:52 INFO 139712694728512] Epoch[111] Batch [5]#011Speed: 180.27 samples/sec#011loss=1.176956\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] Epoch[111] Batch[10] avg_epoch_loss=1.162347\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=1.14481618404\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] Epoch[111] Batch [10]#011Speed: 174.14 samples/sec#011loss=1.144816\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4469.027996063232, \"sum\": 4469.027996063232, \"min\": 4469.027996063232}}, \"EndTime\": 1597163694.437551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163689.968084}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=144.546338049 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=111, train loss <loss>=1.16234679656\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:54 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:55 INFO 139712694728512] Epoch[112] Batch[0] avg_epoch_loss=1.202351\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=1.20235061646\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:57 INFO 139712694728512] Epoch[112] Batch[5] avg_epoch_loss=1.203192\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=1.20319235325\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:57 INFO 139712694728512] Epoch[112] Batch [5]#011Speed: 180.24 samples/sec#011loss=1.203192\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:58 INFO 139712694728512] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4005.889892578125, \"sum\": 4005.889892578125, \"min\": 4005.889892578125}}, \"EndTime\": 1597163698.444049, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163694.437635}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:58 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.019666275 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:58 INFO 139712694728512] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=112, train loss <loss>=1.13695025444\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:58 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:59 INFO 139712694728512] Epoch[113] Batch[0] avg_epoch_loss=1.120934\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:34:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=1.12093400955\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:01 INFO 139712694728512] Epoch[113] Batch[5] avg_epoch_loss=1.153560\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=1.15356032054\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:01 INFO 139712694728512] Epoch[113] Batch [5]#011Speed: 181.73 samples/sec#011loss=1.153560\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] Epoch[113] Batch[10] avg_epoch_loss=1.139963\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=1.12364683151\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] Epoch[113] Batch [10]#011Speed: 172.67 samples/sec#011loss=1.123647\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4450.026988983154, \"sum\": 4450.026988983154, \"min\": 4450.026988983154}}, \"EndTime\": 1597163702.89464, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163698.444141}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.028303197 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=113, train loss <loss>=1.13996328007\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:02 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:03 INFO 139712694728512] Epoch[114] Batch[0] avg_epoch_loss=0.951392\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=0.951391935349\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:05 INFO 139712694728512] Epoch[114] Batch[5] avg_epoch_loss=1.089583\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=1.08958270152\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:05 INFO 139712694728512] Epoch[114] Batch [5]#011Speed: 180.32 samples/sec#011loss=1.089583\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] Epoch[114] Batch[10] avg_epoch_loss=1.007781\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=0.90961894393\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] Epoch[114] Batch [10]#011Speed: 179.68 samples/sec#011loss=0.909619\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4383.111000061035, \"sum\": 4383.111000061035, \"min\": 4383.111000061035}}, \"EndTime\": 1597163707.278321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163702.894729}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.467132969 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=114, train loss <loss>=1.00778099353\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:07 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_441ffc33-3f5c-4ddb-aab2-173e5887b12b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 118.59488487243652, \"sum\": 118.59488487243652, \"min\": 118.59488487243652}}, \"EndTime\": 1597163707.397518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163707.278405}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:08 INFO 139712694728512] Epoch[115] Batch[0] avg_epoch_loss=1.278303\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=1.27830338478\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:09 INFO 139712694728512] Epoch[115] Batch[5] avg_epoch_loss=1.256333\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=1.25633335114\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:09 INFO 139712694728512] Epoch[115] Batch [5]#011Speed: 180.31 samples/sec#011loss=1.256333\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:11 INFO 139712694728512] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4046.2138652801514, \"sum\": 4046.2138652801514, \"min\": 4046.2138652801514}}, \"EndTime\": 1597163711.443894, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163707.397605}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:11 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.023228781 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:11 INFO 139712694728512] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=115, train loss <loss>=1.2149443984\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:11 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:12 INFO 139712694728512] Epoch[116] Batch[0] avg_epoch_loss=1.288476\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=1.28847587109\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:14 INFO 139712694728512] Epoch[116] Batch[5] avg_epoch_loss=1.122636\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=1.12263552348\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:14 INFO 139712694728512] Epoch[116] Batch [5]#011Speed: 179.38 samples/sec#011loss=1.122636\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] Epoch[116] Batch[10] avg_epoch_loss=1.048220\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=0.958920532465\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] Epoch[116] Batch [10]#011Speed: 166.76 samples/sec#011loss=0.958921\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4549.870014190674, \"sum\": 4549.870014190674, \"min\": 4549.870014190674}}, \"EndTime\": 1597163715.994341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163711.443988}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=141.9781425 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=116, train loss <loss>=1.04821961847\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:15 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:16 INFO 139712694728512] Epoch[117] Batch[0] avg_epoch_loss=1.264751\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=1.26475131512\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:18 INFO 139712694728512] Epoch[117] Batch[5] avg_epoch_loss=1.078014\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=1.07801406582\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:18 INFO 139712694728512] Epoch[117] Batch [5]#011Speed: 170.58 samples/sec#011loss=1.078014\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:20 INFO 139712694728512] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4145.12300491333, \"sum\": 4145.12300491333, \"min\": 4145.12300491333}}, \"EndTime\": 1597163720.139991, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163715.994428}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:20 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.914786034 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:20 INFO 139712694728512] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=117, train loss <loss>=1.11149389148\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:20 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:20 INFO 139712694728512] Epoch[118] Batch[0] avg_epoch_loss=1.317602\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=1.31760179996\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:22 INFO 139712694728512] Epoch[118] Batch[5] avg_epoch_loss=1.129252\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=1.12925249338\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:22 INFO 139712694728512] Epoch[118] Batch [5]#011Speed: 181.32 samples/sec#011loss=1.129252\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] Epoch[118] Batch[10] avg_epoch_loss=1.132226\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=1.13579411507\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] Epoch[118] Batch [10]#011Speed: 177.39 samples/sec#011loss=1.135794\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4421.519994735718, \"sum\": 4421.519994735718, \"min\": 4421.519994735718}}, \"EndTime\": 1597163724.562068, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163720.140087}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.015180329 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=118, train loss <loss>=1.13222595778\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:24 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:25 INFO 139712694728512] Epoch[119] Batch[0] avg_epoch_loss=1.126696\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=1.12669599056\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:27 INFO 139712694728512] Epoch[119] Batch[5] avg_epoch_loss=1.145595\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=1.14559529225\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:27 INFO 139712694728512] Epoch[119] Batch [5]#011Speed: 181.65 samples/sec#011loss=1.145595\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:28 INFO 139712694728512] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4041.677951812744, \"sum\": 4041.677951812744, \"min\": 4041.677951812744}}, \"EndTime\": 1597163728.604271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163724.562154}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:28 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.407081712 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:28 INFO 139712694728512] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=119, train loss <loss>=1.13132395744\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:28 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:29 INFO 139712694728512] Epoch[120] Batch[0] avg_epoch_loss=1.211985\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=1.21198475361\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:31 INFO 139712694728512] Epoch[120] Batch[5] avg_epoch_loss=1.124367\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=1.12436701854\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:31 INFO 139712694728512] Epoch[120] Batch [5]#011Speed: 180.22 samples/sec#011loss=1.124367\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] Epoch[120] Batch[10] avg_epoch_loss=1.083652\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=1.03479454517\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] Epoch[120] Batch [10]#011Speed: 178.30 samples/sec#011loss=1.034795\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4420.289993286133, \"sum\": 4420.289993286133, \"min\": 4420.289993286133}}, \"EndTime\": 1597163733.025126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163728.60436}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=145.009043093 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=120, train loss <loss>=1.08365225792\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] Epoch[121] Batch[0] avg_epoch_loss=1.362716\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=1.36271631718\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:35 INFO 139712694728512] Epoch[121] Batch[5] avg_epoch_loss=1.160479\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=1.16047944625\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:35 INFO 139712694728512] Epoch[121] Batch [5]#011Speed: 181.51 samples/sec#011loss=1.160479\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] Epoch[121] Batch[10] avg_epoch_loss=1.072291\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=0.966464483738\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] Epoch[121] Batch [10]#011Speed: 181.06 samples/sec#011loss=0.966464\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4397.892951965332, \"sum\": 4397.892951965332, \"min\": 4397.892951965332}}, \"EndTime\": 1597163737.423592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163733.025211}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.248770139 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=121, train loss <loss>=1.07229082693\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:37 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:38 INFO 139712694728512] Epoch[122] Batch[0] avg_epoch_loss=1.183027\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=1.18302679062\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:40 INFO 139712694728512] Epoch[122] Batch[5] avg_epoch_loss=1.089324\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=1.08932407697\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:40 INFO 139712694728512] Epoch[122] Batch [5]#011Speed: 179.32 samples/sec#011loss=1.089324\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] Epoch[122] Batch[10] avg_epoch_loss=1.026599\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=0.951329088211\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] Epoch[122] Batch [10]#011Speed: 180.36 samples/sec#011loss=0.951329\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4444.252014160156, \"sum\": 4444.252014160156, \"min\": 4444.252014160156}}, \"EndTime\": 1597163741.868346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163737.423676}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.177425647 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=122, train loss <loss>=1.02659908208\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:41 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:42 INFO 139712694728512] Epoch[123] Batch[0] avg_epoch_loss=0.960872\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=0.960872292519\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:44 INFO 139712694728512] Epoch[123] Batch[5] avg_epoch_loss=1.077084\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=1.07708416382\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:44 INFO 139712694728512] Epoch[123] Batch [5]#011Speed: 176.11 samples/sec#011loss=1.077084\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] Epoch[123] Batch[10] avg_epoch_loss=1.095277\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=1.11710829735\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] Epoch[123] Batch [10]#011Speed: 181.16 samples/sec#011loss=1.117108\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] processed a total of 709 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4806.196928024292, \"sum\": 4806.196928024292, \"min\": 4806.196928024292}}, \"EndTime\": 1597163746.675075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163741.868429}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.513888376 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=123, train loss <loss>=0.994578884915\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:46 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_4931eecc-bc42-4d28-9af4-32bfd4201eea-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 116.45793914794922, \"sum\": 116.45793914794922, \"min\": 116.45793914794922}}, \"EndTime\": 1597163746.792231, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163746.675168}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:47 INFO 139712694728512] Epoch[124] Batch[0] avg_epoch_loss=1.017935\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:47 INFO 139712694728512] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=1.01793527603\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:49 INFO 139712694728512] Epoch[124] Batch[5] avg_epoch_loss=1.103101\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=1.10310101509\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:49 INFO 139712694728512] Epoch[124] Batch [5]#011Speed: 179.10 samples/sec#011loss=1.103101\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] Epoch[124] Batch[10] avg_epoch_loss=1.087969\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=1.06981061697\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] Epoch[124] Batch [10]#011Speed: 179.82 samples/sec#011loss=1.069811\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4389.424085617065, \"sum\": 4389.424085617065, \"min\": 4389.424085617065}}, \"EndTime\": 1597163751.181798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163746.792318}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.357474781 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=124, train loss <loss>=1.08796901595\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:51 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:52 INFO 139712694728512] Epoch[125] Batch[0] avg_epoch_loss=0.831970\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=0.831969976425\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:53 INFO 139712694728512] Epoch[125] Batch[5] avg_epoch_loss=1.062709\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:53 INFO 139712694728512] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=1.06270873547\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:53 INFO 139712694728512] Epoch[125] Batch [5]#011Speed: 179.66 samples/sec#011loss=1.062709\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] Epoch[125] Batch[10] avg_epoch_loss=1.127150\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=1.20447998047\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] Epoch[125] Batch [10]#011Speed: 180.04 samples/sec#011loss=1.204480\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4387.628078460693, \"sum\": 4387.628078460693, \"min\": 4387.628078460693}}, \"EndTime\": 1597163755.570002, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163751.181877}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.609308799 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=125, train loss <loss>=1.12715021047\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:55 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:56 INFO 139712694728512] Epoch[126] Batch[0] avg_epoch_loss=0.968726\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=0.968726098537\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:58 INFO 139712694728512] Epoch[126] Batch[5] avg_epoch_loss=1.088490\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=1.0884898901\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:58 INFO 139712694728512] Epoch[126] Batch [5]#011Speed: 180.25 samples/sec#011loss=1.088490\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:59 INFO 139712694728512] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4007.2238445281982, \"sum\": 4007.2238445281982, \"min\": 4007.2238445281982}}, \"EndTime\": 1597163759.577799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163755.570088}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:59 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.214904874 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:59 INFO 139712694728512] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=126, train loss <loss>=1.10701384544\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:35:59 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:00 INFO 139712694728512] Epoch[127] Batch[0] avg_epoch_loss=1.183292\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=1.18329203129\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:02 INFO 139712694728512] Epoch[127] Batch[5] avg_epoch_loss=1.075896\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=1.0758960247\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:02 INFO 139712694728512] Epoch[127] Batch [5]#011Speed: 179.51 samples/sec#011loss=1.075896\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] Epoch[127] Batch[10] avg_epoch_loss=1.135884\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=1.20786921978\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] Epoch[127] Batch [10]#011Speed: 179.36 samples/sec#011loss=1.207869\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4388.3020877838135, \"sum\": 4388.3020877838135, \"min\": 4388.3020877838135}}, \"EndTime\": 1597163763.966688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163759.577881}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.395949746 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=127, train loss <loss>=1.13588384065\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:03 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:04 INFO 139712694728512] Epoch[128] Batch[0] avg_epoch_loss=1.198479\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=1.19847929478\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:06 INFO 139712694728512] Epoch[128] Batch[5] avg_epoch_loss=1.122977\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=1.12297654152\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:06 INFO 139712694728512] Epoch[128] Batch [5]#011Speed: 180.98 samples/sec#011loss=1.122977\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:07 INFO 139712694728512] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4006.784200668335, \"sum\": 4006.784200668335, \"min\": 4006.784200668335}}, \"EndTime\": 1597163767.974021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163763.966759}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:07 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=159.72359805 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:07 INFO 139712694728512] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=128, train loss <loss>=1.10437335372\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:07 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:08 INFO 139712694728512] Epoch[129] Batch[0] avg_epoch_loss=0.775065\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=0.775065362453\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:10 INFO 139712694728512] Epoch[129] Batch[5] avg_epoch_loss=1.079861\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=1.07986130317\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:10 INFO 139712694728512] Epoch[129] Batch [5]#011Speed: 180.91 samples/sec#011loss=1.079861\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] Epoch[129] Batch[10] avg_epoch_loss=1.100373\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=1.12498614788\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] Epoch[129] Batch [10]#011Speed: 177.61 samples/sec#011loss=1.124986\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4400.467872619629, \"sum\": 4400.467872619629, \"min\": 4400.467872619629}}, \"EndTime\": 1597163772.37514, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163767.974116}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.252272352 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=129, train loss <loss>=1.10037259622\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:12 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:13 INFO 139712694728512] Epoch[130] Batch[0] avg_epoch_loss=0.989237\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=0.989237308502\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:15 INFO 139712694728512] Epoch[130] Batch[5] avg_epoch_loss=1.056514\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=1.05651411414\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:15 INFO 139712694728512] Epoch[130] Batch [5]#011Speed: 179.28 samples/sec#011loss=1.056514\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] Epoch[130] Batch[10] avg_epoch_loss=1.097984\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=1.14774676561\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] Epoch[130] Batch [10]#011Speed: 180.74 samples/sec#011loss=1.147747\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4436.29789352417, \"sum\": 4436.29789352417, \"min\": 4436.29789352417}}, \"EndTime\": 1597163776.812017, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163772.375225}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.514359957 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=130, train loss <loss>=1.09798350117\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:16 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:17 INFO 139712694728512] Epoch[131] Batch[0] avg_epoch_loss=1.250832\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=1.25083243847\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:19 INFO 139712694728512] Epoch[131] Batch[5] avg_epoch_loss=1.100389\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=1.10038852692\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:19 INFO 139712694728512] Epoch[131] Batch [5]#011Speed: 180.30 samples/sec#011loss=1.100389\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] Epoch[131] Batch[10] avg_epoch_loss=1.119603\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=1.14266039133\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] Epoch[131] Batch [10]#011Speed: 179.99 samples/sec#011loss=1.142660\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4398.138999938965, \"sum\": 4398.138999938965, \"min\": 4398.138999938965}}, \"EndTime\": 1597163781.210685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163776.812106}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.013009789 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=131, train loss <loss>=1.11960301074\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:21 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:22 INFO 139712694728512] Epoch[132] Batch[0] avg_epoch_loss=1.073118\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=1.07311820984\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:23 INFO 139712694728512] Epoch[132] Batch[5] avg_epoch_loss=1.133659\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=1.13365872701\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:23 INFO 139712694728512] Epoch[132] Batch [5]#011Speed: 182.67 samples/sec#011loss=1.133659\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:25 INFO 139712694728512] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4003.7310123443604, \"sum\": 4003.7310123443604, \"min\": 4003.7310123443604}}, \"EndTime\": 1597163785.214937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163781.210772}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:25 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.850338802 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:25 INFO 139712694728512] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=132, train loss <loss>=1.12240304947\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:25 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:26 INFO 139712694728512] Epoch[133] Batch[0] avg_epoch_loss=1.084483\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=1.08448255062\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:27 INFO 139712694728512] Epoch[133] Batch[5] avg_epoch_loss=1.082496\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=1.08249648412\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:27 INFO 139712694728512] Epoch[133] Batch [5]#011Speed: 183.42 samples/sec#011loss=1.082496\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:29 INFO 139712694728512] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3977.8921604156494, \"sum\": 3977.8921604156494, \"min\": 3977.8921604156494}}, \"EndTime\": 1597163789.193491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163785.215032}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:29 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.068868637 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:29 INFO 139712694728512] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=133, train loss <loss>=1.10553750992\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:29 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:30 INFO 139712694728512] Epoch[134] Batch[0] avg_epoch_loss=0.975010\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:30 INFO 139712694728512] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=0.975010037422\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:31 INFO 139712694728512] Epoch[134] Batch[5] avg_epoch_loss=1.114646\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=1.11464581887\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:31 INFO 139712694728512] Epoch[134] Batch [5]#011Speed: 181.84 samples/sec#011loss=1.114646\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:33 INFO 139712694728512] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4000.822067260742, \"sum\": 4000.822067260742, \"min\": 4000.822067260742}}, \"EndTime\": 1597163793.194927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163789.193584}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:33 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=159.961766798 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:33 INFO 139712694728512] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=134, train loss <loss>=1.1143671155\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:33 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:34 INFO 139712694728512] Epoch[135] Batch[0] avg_epoch_loss=1.213012\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:34 INFO 139712694728512] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=1.21301245689\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:35 INFO 139712694728512] Epoch[135] Batch[5] avg_epoch_loss=1.158444\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=1.15844426552\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:35 INFO 139712694728512] Epoch[135] Batch [5]#011Speed: 180.91 samples/sec#011loss=1.158444\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:37 INFO 139712694728512] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4017.0981884002686, \"sum\": 4017.0981884002686, \"min\": 4017.0981884002686}}, \"EndTime\": 1597163797.212634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163793.195021}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:37 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.605501697 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:37 INFO 139712694728512] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=135, train loss <loss>=1.04065063596\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:37 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:38 INFO 139712694728512] Epoch[136] Batch[0] avg_epoch_loss=1.185432\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=1.18543207645\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:39 INFO 139712694728512] Epoch[136] Batch[5] avg_epoch_loss=1.153148\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=1.15314759811\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:39 INFO 139712694728512] Epoch[136] Batch [5]#011Speed: 180.78 samples/sec#011loss=1.153148\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] Epoch[136] Batch[10] avg_epoch_loss=1.260671\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=1.38969819546\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] Epoch[136] Batch [10]#011Speed: 180.31 samples/sec#011loss=1.389698\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4406.193017959595, \"sum\": 4406.193017959595, \"min\": 4406.193017959595}}, \"EndTime\": 1597163801.619392, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163797.212729}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.648903014 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=136, train loss <loss>=1.2606705969\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:41 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:42 INFO 139712694728512] Epoch[137] Batch[0] avg_epoch_loss=1.045846\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=1.04584574699\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:44 INFO 139712694728512] Epoch[137] Batch[5] avg_epoch_loss=1.130114\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=1.13011406859\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:44 INFO 139712694728512] Epoch[137] Batch [5]#011Speed: 181.40 samples/sec#011loss=1.130114\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] Epoch[137] Batch[10] avg_epoch_loss=1.096664\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=1.05652397275\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] Epoch[137] Batch [10]#011Speed: 176.44 samples/sec#011loss=1.056524\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4432.326078414917, \"sum\": 4432.326078414917, \"min\": 4432.326078414917}}, \"EndTime\": 1597163806.052287, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163801.619518}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.676238122 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=137, train loss <loss>=1.09666402502\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] Epoch[138] Batch[0] avg_epoch_loss=1.262230\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=1.26222980022\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:48 INFO 139712694728512] Epoch[138] Batch[5] avg_epoch_loss=1.168587\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=1.16858728727\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:48 INFO 139712694728512] Epoch[138] Batch [5]#011Speed: 179.39 samples/sec#011loss=1.168587\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:50 INFO 139712694728512] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4017.7340507507324, \"sum\": 4017.7340507507324, \"min\": 4017.7340507507324}}, \"EndTime\": 1597163810.070544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163806.052373}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:50 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.323926198 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:50 INFO 139712694728512] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=138, train loss <loss>=1.12150884867\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:50 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:50 INFO 139712694728512] Epoch[139] Batch[0] avg_epoch_loss=1.034034\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=1.03403353691\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:52 INFO 139712694728512] Epoch[139] Batch[5] avg_epoch_loss=1.127811\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=1.12781081597\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:52 INFO 139712694728512] Epoch[139] Batch [5]#011Speed: 180.04 samples/sec#011loss=1.127811\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:54 INFO 139712694728512] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4066.7788982391357, \"sum\": 4066.7788982391357, \"min\": 4066.7788982391357}}, \"EndTime\": 1597163814.137889, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163810.07064}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:54 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.646313851 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:54 INFO 139712694728512] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=139, train loss <loss>=1.15589573383\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:54 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:54 INFO 139712694728512] Epoch[140] Batch[0] avg_epoch_loss=1.087171\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=1.0871707201\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:56 INFO 139712694728512] Epoch[140] Batch[5] avg_epoch_loss=1.079726\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=1.07972589135\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:56 INFO 139712694728512] Epoch[140] Batch [5]#011Speed: 182.31 samples/sec#011loss=1.079726\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:58 INFO 139712694728512] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3985.2850437164307, \"sum\": 3985.2850437164307, \"min\": 3985.2850437164307}}, \"EndTime\": 1597163818.123753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163814.137983}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:58 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.316164893 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:58 INFO 139712694728512] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=140, train loss <loss>=1.08038576245\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:58 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:58 INFO 139712694728512] Epoch[141] Batch[0] avg_epoch_loss=1.211944\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:36:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=1.21194434166\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:00 INFO 139712694728512] Epoch[141] Batch[5] avg_epoch_loss=1.039579\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=1.03957897425\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:00 INFO 139712694728512] Epoch[141] Batch [5]#011Speed: 179.53 samples/sec#011loss=1.039579\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:02 INFO 139712694728512] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4123.619079589844, \"sum\": 4123.619079589844, \"min\": 4123.619079589844}}, \"EndTime\": 1597163822.247956, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163818.123848}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:02 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.228403591 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:02 INFO 139712694728512] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=141, train loss <loss>=1.08522256613\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:02 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:03 INFO 139712694728512] Epoch[142] Batch[0] avg_epoch_loss=1.163422\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=1.16342246532\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:04 INFO 139712694728512] Epoch[142] Batch[5] avg_epoch_loss=1.090607\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=1.09060733517\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:04 INFO 139712694728512] Epoch[142] Batch [5]#011Speed: 177.65 samples/sec#011loss=1.090607\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:06 INFO 139712694728512] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4044.901132583618, \"sum\": 4044.901132583618, \"min\": 4044.901132583618}}, \"EndTime\": 1597163826.293483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163822.248048}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:06 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.97148078 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:06 INFO 139712694728512] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=142, train loss <loss>=1.09693942666\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:06 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:07 INFO 139712694728512] Epoch[143] Batch[0] avg_epoch_loss=1.001300\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=1.00129961967\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:08 INFO 139712694728512] Epoch[143] Batch[5] avg_epoch_loss=0.982782\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=0.982781648636\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:08 INFO 139712694728512] Epoch[143] Batch [5]#011Speed: 181.25 samples/sec#011loss=0.982782\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:10 INFO 139712694728512] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4091.2649631500244, \"sum\": 4091.2649631500244, \"min\": 4091.2649631500244}}, \"EndTime\": 1597163830.385334, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163826.293575}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:10 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.048570642 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:10 INFO 139712694728512] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=143, train loss <loss>=0.964668959379\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:10 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:10 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_52f48f7b-e5cd-457d-a540-b7d92bc952fc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 121.96683883666992, \"sum\": 121.96683883666992, \"min\": 121.96683883666992}}, \"EndTime\": 1597163830.507921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163830.385428}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:11 INFO 139712694728512] Epoch[144] Batch[0] avg_epoch_loss=1.061467\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=1.06146657467\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:13 INFO 139712694728512] Epoch[144] Batch[5] avg_epoch_loss=1.060526\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=1.0605255266\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:13 INFO 139712694728512] Epoch[144] Batch [5]#011Speed: 181.14 samples/sec#011loss=1.060526\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] Epoch[144] Batch[10] avg_epoch_loss=1.008227\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=0.94546970129\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] Epoch[144] Batch [10]#011Speed: 174.58 samples/sec#011loss=0.945470\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4430.218935012817, \"sum\": 4430.218935012817, \"min\": 4430.218935012817}}, \"EndTime\": 1597163834.938292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163830.507998}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.680877431 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=144, train loss <loss>=1.00822742419\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:14 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:15 INFO 139712694728512] Epoch[145] Batch[0] avg_epoch_loss=1.230642\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=1.23064184189\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:17 INFO 139712694728512] Epoch[145] Batch[5] avg_epoch_loss=1.126307\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=1.12630742788\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:17 INFO 139712694728512] Epoch[145] Batch [5]#011Speed: 181.64 samples/sec#011loss=1.126307\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:18 INFO 139712694728512] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3988.9328479766846, \"sum\": 3988.9328479766846, \"min\": 3988.9328479766846}}, \"EndTime\": 1597163838.927908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163834.938383}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:18 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.923561863 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:18 INFO 139712694728512] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=145, train loss <loss>=1.10393586159\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:18 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:19 INFO 139712694728512] Epoch[146] Batch[0] avg_epoch_loss=1.125934\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=1.12593364716\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:21 INFO 139712694728512] Epoch[146] Batch[5] avg_epoch_loss=1.125972\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=1.12597193321\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:21 INFO 139712694728512] Epoch[146] Batch [5]#011Speed: 179.15 samples/sec#011loss=1.125972\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:23 INFO 139712694728512] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4081.3910961151123, \"sum\": 4081.3910961151123, \"min\": 4081.3910961151123}}, \"EndTime\": 1597163843.009869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163838.928001}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:23 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.843935978 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:23 INFO 139712694728512] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=146, train loss <loss>=1.12459720373\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:23 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:23 INFO 139712694728512] Epoch[147] Batch[0] avg_epoch_loss=1.136371\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=1.13637065887\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:25 INFO 139712694728512] Epoch[147] Batch[5] avg_epoch_loss=1.094931\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=1.09493062894\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:25 INFO 139712694728512] Epoch[147] Batch [5]#011Speed: 177.95 samples/sec#011loss=1.094931\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] Epoch[147] Batch[10] avg_epoch_loss=1.068235\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=1.03620132208\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] Epoch[147] Batch [10]#011Speed: 178.13 samples/sec#011loss=1.036201\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4425.416946411133, \"sum\": 4425.416946411133, \"min\": 4425.416946411133}}, \"EndTime\": 1597163847.435908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163843.009964}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.783438814 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=147, train loss <loss>=1.06823548946\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:27 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:28 INFO 139712694728512] Epoch[148] Batch[0] avg_epoch_loss=1.111218\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=1.11121797562\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:30 INFO 139712694728512] Epoch[148] Batch[5] avg_epoch_loss=0.932668\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:30 INFO 139712694728512] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=0.932668010394\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:30 INFO 139712694728512] Epoch[148] Batch [5]#011Speed: 179.62 samples/sec#011loss=0.932668\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:31 INFO 139712694728512] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4050.2870082855225, \"sum\": 4050.2870082855225, \"min\": 4050.2870082855225}}, \"EndTime\": 1597163851.486718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163847.435991}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:31 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.79872341 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:31 INFO 139712694728512] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=148, train loss <loss>=0.930877500772\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:31 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:31 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_c379a84c-d9e5-428c-a3ed-440c687e2dd2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 121.64807319641113, \"sum\": 121.64807319641113, \"min\": 121.64807319641113}}, \"EndTime\": 1597163851.609009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163851.486812}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:32 INFO 139712694728512] Epoch[149] Batch[0] avg_epoch_loss=0.866502\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=0.866501629353\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:34 INFO 139712694728512] Epoch[149] Batch[5] avg_epoch_loss=0.998818\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:34 INFO 139712694728512] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=0.998817722003\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:34 INFO 139712694728512] Epoch[149] Batch [5]#011Speed: 183.59 samples/sec#011loss=0.998818\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] Epoch[149] Batch[10] avg_epoch_loss=1.087973\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=1.19495840073\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] Epoch[149] Batch [10]#011Speed: 179.40 samples/sec#011loss=1.194958\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4376.260995864868, \"sum\": 4376.260995864868, \"min\": 4376.260995864868}}, \"EndTime\": 1597163855.985398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163851.609067}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.551600077 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=149, train loss <loss>=1.08797257597\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:35 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:36 INFO 139712694728512] Epoch[150] Batch[0] avg_epoch_loss=1.243900\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=1.24389958382\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:38 INFO 139712694728512] Epoch[150] Batch[5] avg_epoch_loss=1.239033\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=1.23903340101\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:38 INFO 139712694728512] Epoch[150] Batch [5]#011Speed: 180.85 samples/sec#011loss=1.239033\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:39 INFO 139712694728512] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4005.418062210083, \"sum\": 4005.418062210083, \"min\": 4005.418062210083}}, \"EndTime\": 1597163859.991386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163855.985474}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:39 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.784234828 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:39 INFO 139712694728512] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=150, train loss <loss>=1.17977901697\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:39 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:40 INFO 139712694728512] Epoch[151] Batch[0] avg_epoch_loss=1.065940\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=1.06594026089\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:42 INFO 139712694728512] Epoch[151] Batch[5] avg_epoch_loss=1.102599\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=1.10259858767\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:42 INFO 139712694728512] Epoch[151] Batch [5]#011Speed: 181.66 samples/sec#011loss=1.102599\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:44 INFO 139712694728512] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4009.5109939575195, \"sum\": 4009.5109939575195, \"min\": 4009.5109939575195}}, \"EndTime\": 1597163864.001493, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163859.991502}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:44 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.135799572 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:44 INFO 139712694728512] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=151, train loss <loss>=1.04720793962\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:44 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:44 INFO 139712694728512] Epoch[152] Batch[0] avg_epoch_loss=0.987609\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=0.987609267235\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:46 INFO 139712694728512] Epoch[152] Batch[5] avg_epoch_loss=1.072716\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=1.0727164348\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:46 INFO 139712694728512] Epoch[152] Batch [5]#011Speed: 177.43 samples/sec#011loss=1.072716\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] Epoch[152] Batch[10] avg_epoch_loss=1.134112\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=1.20778763294\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] Epoch[152] Batch [10]#011Speed: 177.44 samples/sec#011loss=1.207788\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4469.477891921997, \"sum\": 4469.477891921997, \"min\": 4469.477891921997}}, \"EndTime\": 1597163868.471549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163864.001582}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=145.426846279 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=152, train loss <loss>=1.13411243395\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:48 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:49 INFO 139712694728512] Epoch[153] Batch[0] avg_epoch_loss=1.290933\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=1.29093253613\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:51 INFO 139712694728512] Epoch[153] Batch[5] avg_epoch_loss=1.102671\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=1.10267067949\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:51 INFO 139712694728512] Epoch[153] Batch [5]#011Speed: 179.08 samples/sec#011loss=1.102671\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:52 INFO 139712694728512] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4059.5200061798096, \"sum\": 4059.5200061798096, \"min\": 4059.5200061798096}}, \"EndTime\": 1597163872.531591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163868.471635}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:52 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.954052223 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:52 INFO 139712694728512] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=153, train loss <loss>=1.08038882017\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:52 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:53 INFO 139712694728512] Epoch[154] Batch[0] avg_epoch_loss=0.837958\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:53 INFO 139712694728512] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=0.837958455086\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:55 INFO 139712694728512] Epoch[154] Batch[5] avg_epoch_loss=1.075024\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=1.07502424717\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:55 INFO 139712694728512] Epoch[154] Batch [5]#011Speed: 180.64 samples/sec#011loss=1.075024\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] Epoch[154] Batch[10] avg_epoch_loss=1.108284\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=1.14819562435\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] Epoch[154] Batch [10]#011Speed: 180.57 samples/sec#011loss=1.148196\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4463.219881057739, \"sum\": 4463.219881057739, \"min\": 4463.219881057739}}, \"EndTime\": 1597163876.995386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163872.531684}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.526256143 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=154, train loss <loss>=1.10828396407\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:56 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:57 INFO 139712694728512] Epoch[155] Batch[0] avg_epoch_loss=1.362607\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=1.36260712147\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:59 INFO 139712694728512] Epoch[155] Batch[5] avg_epoch_loss=1.090745\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=1.09074544907\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:37:59 INFO 139712694728512] Epoch[155] Batch [5]#011Speed: 182.10 samples/sec#011loss=1.090745\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] Epoch[155] Batch[10] avg_epoch_loss=1.035343\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=0.968860137463\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] Epoch[155] Batch [10]#011Speed: 181.48 samples/sec#011loss=0.968860\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4393.860101699829, \"sum\": 4393.860101699829, \"min\": 4393.860101699829}}, \"EndTime\": 1597163881.38982, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163876.995491}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.612322067 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=155, train loss <loss>=1.0353430347\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:01 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:02 INFO 139712694728512] Epoch[156] Batch[0] avg_epoch_loss=1.083451\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=1.08345079422\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:03 INFO 139712694728512] Epoch[156] Batch[5] avg_epoch_loss=1.027240\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=1.02723978957\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:03 INFO 139712694728512] Epoch[156] Batch [5]#011Speed: 183.35 samples/sec#011loss=1.027240\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:05 INFO 139712694728512] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3991.788148880005, \"sum\": 3991.788148880005, \"min\": 3991.788148880005}}, \"EndTime\": 1597163885.382132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163881.389906}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:05 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.554002639 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:05 INFO 139712694728512] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=156, train loss <loss>=1.03694899678\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:05 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:06 INFO 139712694728512] Epoch[157] Batch[0] avg_epoch_loss=1.186815\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=1.18681454659\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:07 INFO 139712694728512] Epoch[157] Batch[5] avg_epoch_loss=1.050573\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=1.05057329933\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:07 INFO 139712694728512] Epoch[157] Batch [5]#011Speed: 180.74 samples/sec#011loss=1.050573\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] Epoch[157] Batch[10] avg_epoch_loss=1.028537\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=1.00209434032\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] Epoch[157] Batch [10]#011Speed: 178.62 samples/sec#011loss=1.002094\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4398.275852203369, \"sum\": 4398.275852203369, \"min\": 4398.275852203369}}, \"EndTime\": 1597163889.781011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163885.382226}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.467116431 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=157, train loss <loss>=1.02853740887\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:09 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:10 INFO 139712694728512] Epoch[158] Batch[0] avg_epoch_loss=1.066221\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=1.06622099876\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:12 INFO 139712694728512] Epoch[158] Batch[5] avg_epoch_loss=1.115412\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=1.11541218559\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:12 INFO 139712694728512] Epoch[158] Batch [5]#011Speed: 180.22 samples/sec#011loss=1.115412\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:13 INFO 139712694728512] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4007.434129714966, \"sum\": 4007.434129714966, \"min\": 4007.434129714966}}, \"EndTime\": 1597163893.789016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163889.781084}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:13 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.705363621 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:13 INFO 139712694728512] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=158, train loss <loss>=1.09911943078\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:13 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:14 INFO 139712694728512] Epoch[159] Batch[0] avg_epoch_loss=1.113114\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=1.11311411858\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:16 INFO 139712694728512] Epoch[159] Batch[5] avg_epoch_loss=1.039195\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=1.03919541836\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:16 INFO 139712694728512] Epoch[159] Batch [5]#011Speed: 177.59 samples/sec#011loss=1.039195\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:17 INFO 139712694728512] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4073.0531215667725, \"sum\": 4073.0531215667725, \"min\": 4073.0531215667725}}, \"EndTime\": 1597163897.862629, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163893.789111}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:17 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=143.622714918 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:17 INFO 139712694728512] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:17 INFO 139712694728512] #quality_metric: host=algo-1, epoch=159, train loss <loss>=1.07895981073\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:17 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:18 INFO 139712694728512] Epoch[160] Batch[0] avg_epoch_loss=0.895923\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=0.895922780037\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:20 INFO 139712694728512] Epoch[160] Batch[5] avg_epoch_loss=0.987843\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=0.987843394279\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:20 INFO 139712694728512] Epoch[160] Batch [5]#011Speed: 177.77 samples/sec#011loss=0.987843\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] Epoch[160] Batch[10] avg_epoch_loss=1.050359\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=1.12537841797\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] Epoch[160] Batch [10]#011Speed: 179.19 samples/sec#011loss=1.125378\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4422.461986541748, \"sum\": 4422.461986541748, \"min\": 4422.461986541748}}, \"EndTime\": 1597163902.285703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163897.862706}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.947259885 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=160, train loss <loss>=1.05035931414\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:22 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:23 INFO 139712694728512] Epoch[161] Batch[0] avg_epoch_loss=0.960754\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=0.960753858089\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:24 INFO 139712694728512] Epoch[161] Batch[5] avg_epoch_loss=1.071934\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=1.0719344914\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:24 INFO 139712694728512] Epoch[161] Batch [5]#011Speed: 178.22 samples/sec#011loss=1.071934\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] Epoch[161] Batch[10] avg_epoch_loss=1.046453\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=1.01587486267\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] Epoch[161] Batch [10]#011Speed: 180.03 samples/sec#011loss=1.015875\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4404.755115509033, \"sum\": 4404.755115509033, \"min\": 4404.755115509033}}, \"EndTime\": 1597163906.690998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163902.285791}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.46108645 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=161, train loss <loss>=1.04645284198\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:26 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:27 INFO 139712694728512] Epoch[162] Batch[0] avg_epoch_loss=1.299434\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=1.29943394661\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:29 INFO 139712694728512] Epoch[162] Batch[5] avg_epoch_loss=1.060289\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=1.06028879682\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:29 INFO 139712694728512] Epoch[162] Batch [5]#011Speed: 180.97 samples/sec#011loss=1.060289\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] Epoch[162] Batch[10] avg_epoch_loss=1.089356\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=1.12423727512\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] Epoch[162] Batch [10]#011Speed: 177.15 samples/sec#011loss=1.124237\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4390.190839767456, \"sum\": 4390.190839767456, \"min\": 4390.190839767456}}, \"EndTime\": 1597163911.081762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163906.691076}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.003252688 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=162, train loss <loss>=1.08935628696\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] Epoch[163] Batch[0] avg_epoch_loss=1.010671\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=1.01067078114\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:33 INFO 139712694728512] Epoch[163] Batch[5] avg_epoch_loss=1.067133\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=1.06713275115\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:33 INFO 139712694728512] Epoch[163] Batch [5]#011Speed: 181.11 samples/sec#011loss=1.067133\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] Epoch[163] Batch[10] avg_epoch_loss=1.016856\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=0.956523561478\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] Epoch[163] Batch [10]#011Speed: 180.01 samples/sec#011loss=0.956524\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4367.375135421753, \"sum\": 4367.375135421753, \"min\": 4367.375135421753}}, \"EndTime\": 1597163915.449697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163911.081844}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.200242758 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=163, train loss <loss>=1.01685584675\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:35 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:36 INFO 139712694728512] Epoch[164] Batch[0] avg_epoch_loss=1.030447\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=1.03044748306\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:38 INFO 139712694728512] Epoch[164] Batch[5] avg_epoch_loss=1.162939\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=1.16293907166\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:38 INFO 139712694728512] Epoch[164] Batch [5]#011Speed: 180.53 samples/sec#011loss=1.162939\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:39 INFO 139712694728512] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4054.3642044067383, \"sum\": 4054.3642044067383, \"min\": 4054.3642044067383}}, \"EndTime\": 1597163919.504621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163915.449785}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:39 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.889624873 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:39 INFO 139712694728512] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=164, train loss <loss>=1.05023958683\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:39 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:40 INFO 139712694728512] Epoch[165] Batch[0] avg_epoch_loss=1.208059\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=1.2080591917\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:42 INFO 139712694728512] Epoch[165] Batch[5] avg_epoch_loss=1.056523\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=1.05652261774\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:42 INFO 139712694728512] Epoch[165] Batch [5]#011Speed: 179.37 samples/sec#011loss=1.056523\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:43 INFO 139712694728512] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4114.4468784332275, \"sum\": 4114.4468784332275, \"min\": 4114.4468784332275}}, \"EndTime\": 1597163923.619665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163919.504716}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:43 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.114042346 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:43 INFO 139712694728512] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=165, train loss <loss>=1.1208927691\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:43 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:44 INFO 139712694728512] Epoch[166] Batch[0] avg_epoch_loss=1.115068\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=1.11506772041\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:46 INFO 139712694728512] Epoch[166] Batch[5] avg_epoch_loss=0.948985\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=0.948985050122\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:46 INFO 139712694728512] Epoch[166] Batch [5]#011Speed: 176.21 samples/sec#011loss=0.948985\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:47 INFO 139712694728512] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4055.5639266967773, \"sum\": 4055.5639266967773, \"min\": 4055.5639266967773}}, \"EndTime\": 1597163927.675845, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163923.619757}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:47 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.145903498 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:47 INFO 139712694728512] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:47 INFO 139712694728512] #quality_metric: host=algo-1, epoch=166, train loss <loss>=1.12000430822\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:47 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:48 INFO 139712694728512] Epoch[167] Batch[0] avg_epoch_loss=1.035783\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=1.03578341007\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:50 INFO 139712694728512] Epoch[167] Batch[5] avg_epoch_loss=1.136900\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=1.13690026601\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:50 INFO 139712694728512] Epoch[167] Batch [5]#011Speed: 178.29 samples/sec#011loss=1.136900\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:51 INFO 139712694728512] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4069.4432258605957, \"sum\": 4069.4432258605957, \"min\": 4069.4432258605957}}, \"EndTime\": 1597163931.745866, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163927.675931}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:51 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.172535845 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:51 INFO 139712694728512] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=167, train loss <loss>=1.12142962217\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:51 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:52 INFO 139712694728512] Epoch[168] Batch[0] avg_epoch_loss=0.930664\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=0.930664300919\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:54 INFO 139712694728512] Epoch[168] Batch[5] avg_epoch_loss=1.074099\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=1.07409928242\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:54 INFO 139712694728512] Epoch[168] Batch [5]#011Speed: 180.34 samples/sec#011loss=1.074099\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:55 INFO 139712694728512] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4040.0497913360596, \"sum\": 4040.0497913360596, \"min\": 4040.0497913360596}}, \"EndTime\": 1597163935.786544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163931.745962}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:55 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.181087755 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:55 INFO 139712694728512] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=168, train loss <loss>=1.0590359509\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:55 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:56 INFO 139712694728512] Epoch[169] Batch[0] avg_epoch_loss=1.102757\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:56 INFO 139712694728512] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=1.10275709629\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:58 INFO 139712694728512] Epoch[169] Batch[5] avg_epoch_loss=1.041215\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=1.04121523102\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:58 INFO 139712694728512] Epoch[169] Batch [5]#011Speed: 183.30 samples/sec#011loss=1.041215\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:59 INFO 139712694728512] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3970.13521194458, \"sum\": 3970.13521194458, \"min\": 3970.13521194458}}, \"EndTime\": 1597163939.757234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163935.786638}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:59 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=159.938988118 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:59 INFO 139712694728512] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=169, train loss <loss>=1.03200520873\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:38:59 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:00 INFO 139712694728512] Epoch[170] Batch[0] avg_epoch_loss=1.034919\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=1.03491914272\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:02 INFO 139712694728512] Epoch[170] Batch[5] avg_epoch_loss=0.993256\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=0.99325616161\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:02 INFO 139712694728512] Epoch[170] Batch [5]#011Speed: 178.79 samples/sec#011loss=0.993256\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:03 INFO 139712694728512] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4018.5458660125732, \"sum\": 4018.5458660125732, \"min\": 4018.5458660125732}}, \"EndTime\": 1597163943.776356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163939.757324}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:03 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.290135772 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:03 INFO 139712694728512] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=170, train loss <loss>=0.939488351345\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:03 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:04 INFO 139712694728512] Epoch[171] Batch[0] avg_epoch_loss=0.963876\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=0.963875830173\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:06 INFO 139712694728512] Epoch[171] Batch[5] avg_epoch_loss=0.970258\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=0.970258265734\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:06 INFO 139712694728512] Epoch[171] Batch [5]#011Speed: 180.26 samples/sec#011loss=0.970258\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] Epoch[171] Batch[10] avg_epoch_loss=0.992003\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=1.01809661388\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] Epoch[171] Batch [10]#011Speed: 181.65 samples/sec#011loss=1.018097\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4350.077152252197, \"sum\": 4350.077152252197, \"min\": 4350.077152252197}}, \"EndTime\": 1597163948.12699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163943.776424}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.31472297 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=171, train loss <loss>=0.992002969438\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] Epoch[172] Batch[0] avg_epoch_loss=0.905280\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=0.90528023243\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:10 INFO 139712694728512] Epoch[172] Batch[5] avg_epoch_loss=1.026230\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=1.02623049418\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:10 INFO 139712694728512] Epoch[172] Batch [5]#011Speed: 180.95 samples/sec#011loss=1.026230\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:12 INFO 139712694728512] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3977.8101444244385, \"sum\": 3977.8101444244385, \"min\": 3977.8101444244385}}, \"EndTime\": 1597163952.105376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163948.127071}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:12 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.373388208 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:12 INFO 139712694728512] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=172, train loss <loss>=1.02539645433\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:12 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:12 INFO 139712694728512] Epoch[173] Batch[0] avg_epoch_loss=1.213286\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=1.21328556538\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:14 INFO 139712694728512] Epoch[173] Batch[5] avg_epoch_loss=1.055589\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=1.05558893085\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:14 INFO 139712694728512] Epoch[173] Batch [5]#011Speed: 182.66 samples/sec#011loss=1.055589\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:16 INFO 139712694728512] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4031.8119525909424, \"sum\": 4031.8119525909424, \"min\": 4031.8119525909424}}, \"EndTime\": 1597163956.137755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163952.105467}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:16 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.269236621 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:16 INFO 139712694728512] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=173, train loss <loss>=1.04760062099\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:16 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:16 INFO 139712694728512] Epoch[174] Batch[0] avg_epoch_loss=0.919413\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=0.919413030148\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:18 INFO 139712694728512] Epoch[174] Batch[5] avg_epoch_loss=1.028444\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=1.02844401201\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:18 INFO 139712694728512] Epoch[174] Batch [5]#011Speed: 181.64 samples/sec#011loss=1.028444\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] Epoch[174] Batch[10] avg_epoch_loss=1.021873\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=1.01398808956\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] Epoch[174] Batch [10]#011Speed: 181.08 samples/sec#011loss=1.013988\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4335.93487739563, \"sum\": 4335.93487739563, \"min\": 4335.93487739563}}, \"EndTime\": 1597163960.474401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163956.137824}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.291336935 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=174, train loss <loss>=1.02187313817\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:20 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:21 INFO 139712694728512] Epoch[175] Batch[0] avg_epoch_loss=1.021686\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=1.02168643475\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:23 INFO 139712694728512] Epoch[175] Batch[5] avg_epoch_loss=1.128062\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:23 INFO 139712694728512] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=1.1280618906\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:23 INFO 139712694728512] Epoch[175] Batch [5]#011Speed: 181.36 samples/sec#011loss=1.128062\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] Epoch[175] Batch[10] avg_epoch_loss=1.115168\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=1.09969438314\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] Epoch[175] Batch [10]#011Speed: 180.66 samples/sec#011loss=1.099694\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4361.0711097717285, \"sum\": 4361.0711097717285, \"min\": 4361.0711097717285}}, \"EndTime\": 1597163964.836071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163960.474487}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.435974403 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=175, train loss <loss>=1.11516756903\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:24 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:25 INFO 139712694728512] Epoch[176] Batch[0] avg_epoch_loss=1.041078\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=1.04107785225\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:27 INFO 139712694728512] Epoch[176] Batch[5] avg_epoch_loss=1.107060\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=1.1070599854\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:27 INFO 139712694728512] Epoch[176] Batch [5]#011Speed: 181.86 samples/sec#011loss=1.107060\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:28 INFO 139712694728512] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4027.4009704589844, \"sum\": 4027.4009704589844, \"min\": 4027.4009704589844}}, \"EndTime\": 1597163968.863996, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163964.836156}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:28 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.919730483 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:28 INFO 139712694728512] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=176, train loss <loss>=1.0654063046\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:28 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:29 INFO 139712694728512] Epoch[177] Batch[0] avg_epoch_loss=1.024301\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=1.02430081367\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:31 INFO 139712694728512] Epoch[177] Batch[5] avg_epoch_loss=0.943426\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=0.943426311016\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:31 INFO 139712694728512] Epoch[177] Batch [5]#011Speed: 180.01 samples/sec#011loss=0.943426\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] Epoch[177] Batch[10] avg_epoch_loss=0.902268\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=0.852879041433\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] Epoch[177] Batch [10]#011Speed: 180.36 samples/sec#011loss=0.852879\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4393.492937088013, \"sum\": 4393.492937088013, \"min\": 4393.492937088013}}, \"EndTime\": 1597163973.258053, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163968.864089}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.720964937 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=177, train loss <loss>=0.902268461206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:33 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/state_1c2b5bd2-ba62-421c-8413-efdaae842811-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 125.34093856811523, \"sum\": 125.34093856811523, \"min\": 125.34093856811523}}, \"EndTime\": 1597163973.383988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163973.258158}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:34 INFO 139712694728512] Epoch[178] Batch[0] avg_epoch_loss=0.957317\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:34 INFO 139712694728512] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=0.957316935062\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:35 INFO 139712694728512] Epoch[178] Batch[5] avg_epoch_loss=1.067385\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=1.06738463044\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:35 INFO 139712694728512] Epoch[178] Batch [5]#011Speed: 181.32 samples/sec#011loss=1.067385\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] Epoch[178] Batch[10] avg_epoch_loss=0.954013\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=0.817967532575\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] Epoch[178] Batch [10]#011Speed: 182.41 samples/sec#011loss=0.817968\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4349.648952484131, \"sum\": 4349.648952484131, \"min\": 4349.648952484131}}, \"EndTime\": 1597163977.733766, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163973.384058}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.28380566 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=178, train loss <loss>=0.954013222321\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:37 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:38 INFO 139712694728512] Epoch[179] Batch[0] avg_epoch_loss=0.999042\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:38 INFO 139712694728512] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=0.999042451382\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:40 INFO 139712694728512] Epoch[179] Batch[5] avg_epoch_loss=1.010574\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=1.01057392359\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:40 INFO 139712694728512] Epoch[179] Batch [5]#011Speed: 181.01 samples/sec#011loss=1.010574\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:41 INFO 139712694728512] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4041.0139560699463, \"sum\": 4041.0139560699463, \"min\": 4041.0139560699463}}, \"EndTime\": 1597163981.775283, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163977.733849}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:41 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.47317172 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:41 INFO 139712694728512] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=179, train loss <loss>=1.06186282039\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:41 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:42 INFO 139712694728512] Epoch[180] Batch[0] avg_epoch_loss=1.088253\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=1.0882525444\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:44 INFO 139712694728512] Epoch[180] Batch[5] avg_epoch_loss=1.087007\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:44 INFO 139712694728512] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=1.08700746298\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:44 INFO 139712694728512] Epoch[180] Batch [5]#011Speed: 182.33 samples/sec#011loss=1.087007\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] Epoch[180] Batch[10] avg_epoch_loss=1.126789\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=1.17452721596\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] Epoch[180] Batch [10]#011Speed: 176.59 samples/sec#011loss=1.174527\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4383.725166320801, \"sum\": 4383.725166320801, \"min\": 4383.725166320801}}, \"EndTime\": 1597163986.159624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163981.77537}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.08091055 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=180, train loss <loss>=1.12678916888\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] Epoch[181] Batch[0] avg_epoch_loss=1.109972\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=1.10997247696\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:48 INFO 139712694728512] Epoch[181] Batch[5] avg_epoch_loss=1.038309\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=1.03830943505\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:48 INFO 139712694728512] Epoch[181] Batch [5]#011Speed: 182.48 samples/sec#011loss=1.038309\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] Epoch[181] Batch[10] avg_epoch_loss=1.059190\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=1.08424569368\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] Epoch[181] Batch [10]#011Speed: 179.63 samples/sec#011loss=1.084246\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4339.842081069946, \"sum\": 4339.842081069946, \"min\": 4339.842081069946}}, \"EndTime\": 1597163990.500001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163986.15969}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.001412067 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=181, train loss <loss>=1.05918955261\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:50 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:51 INFO 139712694728512] Epoch[182] Batch[0] avg_epoch_loss=1.084534\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=1.08453416824\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:53 INFO 139712694728512] Epoch[182] Batch[5] avg_epoch_loss=1.037371\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:53 INFO 139712694728512] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=1.03737119834\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:53 INFO 139712694728512] Epoch[182] Batch [5]#011Speed: 181.43 samples/sec#011loss=1.037371\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] Epoch[182] Batch[10] avg_epoch_loss=1.027905\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=1.01654587984\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] Epoch[182] Batch [10]#011Speed: 183.26 samples/sec#011loss=1.016546\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4325.5250453948975, \"sum\": 4325.5250453948975, \"min\": 4325.5250453948975}}, \"EndTime\": 1597163994.826082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163990.500081}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.04061806 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=182, train loss <loss>=1.02790514447\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:54 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:55 INFO 139712694728512] Epoch[183] Batch[0] avg_epoch_loss=0.896489\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=0.896488547325\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:57 INFO 139712694728512] Epoch[183] Batch[5] avg_epoch_loss=0.917252\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=0.917251696189\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:57 INFO 139712694728512] Epoch[183] Batch [5]#011Speed: 180.66 samples/sec#011loss=0.917252\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:58 INFO 139712694728512] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4025.1150131225586, \"sum\": 4025.1150131225586, \"min\": 4025.1150131225586}}, \"EndTime\": 1597163998.851721, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163994.826169}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:58 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.015239133 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:58 INFO 139712694728512] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=183, train loss <loss>=0.931855380535\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:58 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:59 INFO 139712694728512] Epoch[184] Batch[0] avg_epoch_loss=1.089821\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:39:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=1.08982121944\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:01 INFO 139712694728512] Epoch[184] Batch[5] avg_epoch_loss=1.026854\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=1.02685405811\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:01 INFO 139712694728512] Epoch[184] Batch [5]#011Speed: 178.66 samples/sec#011loss=1.026854\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:02 INFO 139712694728512] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4063.302993774414, \"sum\": 4063.302993774414, \"min\": 4063.302993774414}}, \"EndTime\": 1597164002.915645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597163998.851813}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:02 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.517709781 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:02 INFO 139712694728512] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=184, train loss <loss>=1.00953214169\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:02 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:03 INFO 139712694728512] Epoch[185] Batch[0] avg_epoch_loss=0.817370\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=0.817369699478\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:05 INFO 139712694728512] Epoch[185] Batch[5] avg_epoch_loss=0.963928\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=0.963928252459\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:05 INFO 139712694728512] Epoch[185] Batch [5]#011Speed: 180.40 samples/sec#011loss=0.963928\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] Epoch[185] Batch[10] avg_epoch_loss=0.947951\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=0.928778588772\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] Epoch[185] Batch [10]#011Speed: 178.99 samples/sec#011loss=0.928779\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4404.466152191162, \"sum\": 4404.466152191162, \"min\": 4404.466152191162}}, \"EndTime\": 1597164007.320736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164002.915741}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=150.979073261 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=185, train loss <loss>=0.947951132601\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:07 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:08 INFO 139712694728512] Epoch[186] Batch[0] avg_epoch_loss=1.102598\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=1.1025980711\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:09 INFO 139712694728512] Epoch[186] Batch[5] avg_epoch_loss=1.024374\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:09 INFO 139712694728512] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=1.02437448502\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:09 INFO 139712694728512] Epoch[186] Batch [5]#011Speed: 178.88 samples/sec#011loss=1.024374\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:11 INFO 139712694728512] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4067.9571628570557, \"sum\": 4067.9571628570557, \"min\": 4067.9571628570557}}, \"EndTime\": 1597164011.389194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164007.320817}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:11 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.33872553 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:11 INFO 139712694728512] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=186, train loss <loss>=1.03521108031\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:11 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:12 INFO 139712694728512] Epoch[187] Batch[0] avg_epoch_loss=0.912539\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=0.912539243698\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:13 INFO 139712694728512] Epoch[187] Batch[5] avg_epoch_loss=1.024517\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:13 INFO 139712694728512] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=1.02451700966\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:13 INFO 139712694728512] Epoch[187] Batch [5]#011Speed: 183.65 samples/sec#011loss=1.024517\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] Epoch[187] Batch[10] avg_epoch_loss=1.003194\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=0.977605390549\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] Epoch[187] Batch [10]#011Speed: 179.16 samples/sec#011loss=0.977605\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4368.6089515686035, \"sum\": 4368.6089515686035, \"min\": 4368.6089515686035}}, \"EndTime\": 1597164015.758351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164011.389287}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=149.929153272 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=187, train loss <loss>=1.00319354643\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:15 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:16 INFO 139712694728512] Epoch[188] Batch[0] avg_epoch_loss=1.101123\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=1.10112345219\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:18 INFO 139712694728512] Epoch[188] Batch[5] avg_epoch_loss=1.012970\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=1.0129695634\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:18 INFO 139712694728512] Epoch[188] Batch [5]#011Speed: 182.53 samples/sec#011loss=1.012970\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:19 INFO 139712694728512] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4025.8989334106445, \"sum\": 4025.8989334106445, \"min\": 4025.8989334106445}}, \"EndTime\": 1597164019.784751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164015.758435}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:19 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.488894971 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:19 INFO 139712694728512] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=188, train loss <loss>=0.997040706873\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:19 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:20 INFO 139712694728512] Epoch[189] Batch[0] avg_epoch_loss=0.986485\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=0.986484587193\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:22 INFO 139712694728512] Epoch[189] Batch[5] avg_epoch_loss=0.932842\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=0.932842473189\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:22 INFO 139712694728512] Epoch[189] Batch [5]#011Speed: 177.40 samples/sec#011loss=0.932842\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] Epoch[189] Batch[10] avg_epoch_loss=1.053452\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=1.19818366766\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] Epoch[189] Batch [10]#011Speed: 180.35 samples/sec#011loss=1.198184\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4391.142845153809, \"sum\": 4391.142845153809, \"min\": 4391.142845153809}}, \"EndTime\": 1597164024.176518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164019.784823}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=145.971639505 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=189, train loss <loss>=1.05345210704\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:24 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:25 INFO 139712694728512] Epoch[190] Batch[0] avg_epoch_loss=1.296378\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=1.29637849331\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:26 INFO 139712694728512] Epoch[190] Batch[5] avg_epoch_loss=1.057351\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:26 INFO 139712694728512] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=1.05735085408\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:26 INFO 139712694728512] Epoch[190] Batch [5]#011Speed: 180.15 samples/sec#011loss=1.057351\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] Epoch[190] Batch[10] avg_epoch_loss=1.074980\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=1.09613595009\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] Epoch[190] Batch [10]#011Speed: 179.38 samples/sec#011loss=1.096136\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4405.272960662842, \"sum\": 4405.272960662842, \"min\": 4405.272960662842}}, \"EndTime\": 1597164028.582292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164024.176601}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.865304998 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=190, train loss <loss>=1.07498044317\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:28 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:29 INFO 139712694728512] Epoch[191] Batch[0] avg_epoch_loss=1.044551\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=1.04455065727\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:31 INFO 139712694728512] Epoch[191] Batch[5] avg_epoch_loss=1.050847\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=1.05084749063\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:31 INFO 139712694728512] Epoch[191] Batch [5]#011Speed: 178.38 samples/sec#011loss=1.050847\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] Epoch[191] Batch[10] avg_epoch_loss=1.076939\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=1.10824824572\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] Epoch[191] Batch [10]#011Speed: 179.56 samples/sec#011loss=1.108248\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4461.171865463257, \"sum\": 4461.171865463257, \"min\": 4461.171865463257}}, \"EndTime\": 1597164033.044041, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164028.582379}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.387347929 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=191, train loss <loss>=1.07693874294\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] Epoch[192] Batch[0] avg_epoch_loss=1.137347\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=1.13734698296\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:35 INFO 139712694728512] Epoch[192] Batch[5] avg_epoch_loss=1.076685\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=1.07668491205\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:35 INFO 139712694728512] Epoch[192] Batch [5]#011Speed: 182.26 samples/sec#011loss=1.076685\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:37 INFO 139712694728512] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4048.314094543457, \"sum\": 4048.314094543457, \"min\": 4048.314094543457}}, \"EndTime\": 1597164037.092875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164033.044128}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:37 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=145.981830138 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:37 INFO 139712694728512] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=192, train loss <loss>=1.01706205606\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:37 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:37 INFO 139712694728512] Epoch[193] Batch[0] avg_epoch_loss=1.154598\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=1.15459787846\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:39 INFO 139712694728512] Epoch[193] Batch[5] avg_epoch_loss=1.054968\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=1.054968069\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:39 INFO 139712694728512] Epoch[193] Batch [5]#011Speed: 181.60 samples/sec#011loss=1.054968\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:41 INFO 139712694728512] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4060.5030059814453, \"sum\": 4060.5030059814453, \"min\": 4060.5030059814453}}, \"EndTime\": 1597164041.153965, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164037.092971}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:41 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.005639861 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:41 INFO 139712694728512] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=193, train loss <loss>=1.07903943658\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:41 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:42 INFO 139712694728512] Epoch[194] Batch[0] avg_epoch_loss=0.867496\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:42 INFO 139712694728512] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=0.867496132851\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:43 INFO 139712694728512] Epoch[194] Batch[5] avg_epoch_loss=1.096619\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=1.09661877155\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:43 INFO 139712694728512] Epoch[194] Batch [5]#011Speed: 180.86 samples/sec#011loss=1.096619\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] Epoch[194] Batch[10] avg_epoch_loss=1.084570\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=1.07011108398\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] Epoch[194] Batch [10]#011Speed: 179.31 samples/sec#011loss=1.070111\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4427.189111709595, \"sum\": 4427.189111709595, \"min\": 4427.189111709595}}, \"EndTime\": 1597164045.581731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164041.154059}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.947361165 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=194, train loss <loss>=1.08456982266\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:45 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:46 INFO 139712694728512] Epoch[195] Batch[0] avg_epoch_loss=1.030645\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=1.03064501286\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:48 INFO 139712694728512] Epoch[195] Batch[5] avg_epoch_loss=1.105498\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:48 INFO 139712694728512] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=1.1054983139\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:48 INFO 139712694728512] Epoch[195] Batch [5]#011Speed: 179.83 samples/sec#011loss=1.105498\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] Epoch[195] Batch[10] avg_epoch_loss=1.083939\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=1.05806722641\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] Epoch[195] Batch [10]#011Speed: 182.01 samples/sec#011loss=1.058067\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4781.664133071899, \"sum\": 4781.664133071899, \"min\": 4781.664133071899}}, \"EndTime\": 1597164050.363912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164045.581816}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.06158305 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=195, train loss <loss>=1.12307796876\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:50 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:51 INFO 139712694728512] Epoch[196] Batch[0] avg_epoch_loss=0.858487\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=0.858487010002\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:52 INFO 139712694728512] Epoch[196] Batch[5] avg_epoch_loss=0.974821\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:52 INFO 139712694728512] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=0.974821339051\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:52 INFO 139712694728512] Epoch[196] Batch [5]#011Speed: 179.77 samples/sec#011loss=0.974821\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] Epoch[196] Batch[10] avg_epoch_loss=0.908498\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=0.828909355402\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] Epoch[196] Batch [10]#011Speed: 177.46 samples/sec#011loss=0.828909\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4435.835123062134, \"sum\": 4435.835123062134, \"min\": 4435.835123062134}}, \"EndTime\": 1597164054.800312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164050.364004}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=144.95167988 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=196, train loss <loss>=0.90849771012\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:54 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:55 INFO 139712694728512] Epoch[197] Batch[0] avg_epoch_loss=0.882081\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=0.882080972195\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:57 INFO 139712694728512] Epoch[197] Batch[5] avg_epoch_loss=0.997323\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=0.997322966655\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:57 INFO 139712694728512] Epoch[197] Batch [5]#011Speed: 180.70 samples/sec#011loss=0.997323\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] Epoch[197] Batch[10] avg_epoch_loss=0.916138\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=0.818716937304\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] Epoch[197] Batch [10]#011Speed: 180.12 samples/sec#011loss=0.818717\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4395.49994468689, \"sum\": 4395.49994468689, \"min\": 4395.49994468689}}, \"EndTime\": 1597164059.196328, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164054.8004}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.28679511 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] #quality_metric: host=algo-1, epoch=197, train loss <loss>=0.916138407859\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:40:59 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:00 INFO 139712694728512] Epoch[198] Batch[0] avg_epoch_loss=0.927379\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=0.927379310131\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:01 INFO 139712694728512] Epoch[198] Batch[5] avg_epoch_loss=1.009522\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=1.00952211022\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:01 INFO 139712694728512] Epoch[198] Batch [5]#011Speed: 180.24 samples/sec#011loss=1.009522\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:03 INFO 139712694728512] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4052.865982055664, \"sum\": 4052.865982055664, \"min\": 4052.865982055664}}, \"EndTime\": 1597164063.249713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164059.196414}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:03 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.180644736 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:03 INFO 139712694728512] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:03 INFO 139712694728512] #quality_metric: host=algo-1, epoch=198, train loss <loss>=1.09367516637\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:03 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:04 INFO 139712694728512] Epoch[199] Batch[0] avg_epoch_loss=0.823682\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=0.823682308197\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:05 INFO 139712694728512] Epoch[199] Batch[5] avg_epoch_loss=0.953760\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:05 INFO 139712694728512] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=0.953760455052\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:05 INFO 139712694728512] Epoch[199] Batch [5]#011Speed: 181.91 samples/sec#011loss=0.953760\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] Epoch[199] Batch[10] avg_epoch_loss=0.929314\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=0.899978268147\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] Epoch[199] Batch [10]#011Speed: 181.19 samples/sec#011loss=0.899978\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4379.093885421753, \"sum\": 4379.093885421753, \"min\": 4379.093885421753}}, \"EndTime\": 1597164067.629385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164063.249806}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=155.278937343 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] #quality_metric: host=algo-1, epoch=199, train loss <loss>=0.929314006459\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:07 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:08 INFO 139712694728512] Epoch[200] Batch[0] avg_epoch_loss=1.084446\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=1.08444571495\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:10 INFO 139712694728512] Epoch[200] Batch[5] avg_epoch_loss=1.068774\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=1.06877369682\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:10 INFO 139712694728512] Epoch[200] Batch [5]#011Speed: 182.41 samples/sec#011loss=1.068774\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:11 INFO 139712694728512] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4049.7639179229736, \"sum\": 4049.7639179229736, \"min\": 4049.7639179229736}}, \"EndTime\": 1597164071.679664, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164067.62947}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:11 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.300153439 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:11 INFO 139712694728512] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:11 INFO 139712694728512] #quality_metric: host=algo-1, epoch=200, train loss <loss>=0.986734116077\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:11 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:12 INFO 139712694728512] Epoch[201] Batch[0] avg_epoch_loss=0.971073\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=0.971072673798\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:14 INFO 139712694728512] Epoch[201] Batch[5] avg_epoch_loss=0.941566\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=0.941565781832\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:14 INFO 139712694728512] Epoch[201] Batch [5]#011Speed: 181.39 samples/sec#011loss=0.941566\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:15 INFO 139712694728512] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3995.2151775360107, \"sum\": 3995.2151775360107, \"min\": 3995.2151775360107}}, \"EndTime\": 1597164075.675534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164071.679759}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:15 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=159.935882436 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:15 INFO 139712694728512] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=201, train loss <loss>=0.949440979958\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:15 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:16 INFO 139712694728512] Epoch[202] Batch[0] avg_epoch_loss=0.883315\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=0.883315145969\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:18 INFO 139712694728512] Epoch[202] Batch[5] avg_epoch_loss=0.959714\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=0.959713707368\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:18 INFO 139712694728512] Epoch[202] Batch [5]#011Speed: 180.11 samples/sec#011loss=0.959714\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] Epoch[202] Batch[10] avg_epoch_loss=1.027613\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=202, batch=10 train loss <loss>=1.10909159184\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] Epoch[202] Batch [10]#011Speed: 178.26 samples/sec#011loss=1.109092\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4437.752962112427, \"sum\": 4437.752962112427, \"min\": 4437.752962112427}}, \"EndTime\": 1597164080.113853, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164075.675628}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=145.339704444 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=202, train loss <loss>=1.02761274576\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] Epoch[203] Batch[0] avg_epoch_loss=0.903475\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:20 INFO 139712694728512] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=0.903474748135\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:22 INFO 139712694728512] Epoch[203] Batch[5] avg_epoch_loss=0.970605\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=0.970605144898\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:22 INFO 139712694728512] Epoch[203] Batch [5]#011Speed: 180.43 samples/sec#011loss=0.970605\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] Epoch[203] Batch[10] avg_epoch_loss=1.003135\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=203, batch=10 train loss <loss>=1.04217169285\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] Epoch[203] Batch [10]#011Speed: 179.01 samples/sec#011loss=1.042172\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4402.292013168335, \"sum\": 4402.292013168335, \"min\": 4402.292013168335}}, \"EndTime\": 1597164084.516703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164080.113939}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=153.552016658 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] #quality_metric: host=algo-1, epoch=203, train loss <loss>=1.00313539397\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:24 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:25 INFO 139712694728512] Epoch[204] Batch[0] avg_epoch_loss=1.095183\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:25 INFO 139712694728512] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=1.09518265724\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:27 INFO 139712694728512] Epoch[204] Batch[5] avg_epoch_loss=1.132584\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:27 INFO 139712694728512] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=1.13258386652\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:27 INFO 139712694728512] Epoch[204] Batch [5]#011Speed: 181.16 samples/sec#011loss=1.132584\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:28 INFO 139712694728512] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4015.9919261932373, \"sum\": 4015.9919261932373, \"min\": 4015.9919261932373}}, \"EndTime\": 1597164088.533294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164084.516786}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:28 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=157.366919822 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:28 INFO 139712694728512] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:28 INFO 139712694728512] #quality_metric: host=algo-1, epoch=204, train loss <loss>=1.12465781569\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:28 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:29 INFO 139712694728512] Epoch[205] Batch[0] avg_epoch_loss=1.107947\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:29 INFO 139712694728512] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=1.10794746876\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:31 INFO 139712694728512] Epoch[205] Batch[5] avg_epoch_loss=1.046718\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:31 INFO 139712694728512] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=1.04671797156\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:31 INFO 139712694728512] Epoch[205] Batch [5]#011Speed: 180.24 samples/sec#011loss=1.046718\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] Epoch[205] Batch[10] avg_epoch_loss=1.067479\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=205, batch=10 train loss <loss>=1.09239122868\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] Epoch[205] Batch [10]#011Speed: 180.58 samples/sec#011loss=1.092391\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4376.471042633057, \"sum\": 4376.471042633057, \"min\": 4376.471042633057}}, \"EndTime\": 1597164092.910365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164088.533362}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=156.971847953 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] #quality_metric: host=algo-1, epoch=205, train loss <loss>=1.06747854298\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:32 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:33 INFO 139712694728512] Epoch[206] Batch[0] avg_epoch_loss=1.141160\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:33 INFO 139712694728512] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=1.14116024971\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:35 INFO 139712694728512] Epoch[206] Batch[5] avg_epoch_loss=1.000358\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:35 INFO 139712694728512] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=1.00035795569\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:35 INFO 139712694728512] Epoch[206] Batch [5]#011Speed: 182.26 samples/sec#011loss=1.000358\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:36 INFO 139712694728512] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3984.1370582580566, \"sum\": 3984.1370582580566, \"min\": 3984.1370582580566}}, \"EndTime\": 1597164096.894988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164092.910434}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:36 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.357125303 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:36 INFO 139712694728512] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:36 INFO 139712694728512] #quality_metric: host=algo-1, epoch=206, train loss <loss>=1.02844772935\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:36 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:37 INFO 139712694728512] Epoch[207] Batch[0] avg_epoch_loss=1.069785\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:37 INFO 139712694728512] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=1.0697851181\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:39 INFO 139712694728512] Epoch[207] Batch[5] avg_epoch_loss=1.012930\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:39 INFO 139712694728512] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=1.01292967796\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:39 INFO 139712694728512] Epoch[207] Batch [5]#011Speed: 180.24 samples/sec#011loss=1.012930\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:40 INFO 139712694728512] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4002.8579235076904, \"sum\": 4002.8579235076904, \"min\": 4002.8579235076904}}, \"EndTime\": 1597164100.898526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164096.895078}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:40 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=159.380739105 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:40 INFO 139712694728512] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:40 INFO 139712694728512] #quality_metric: host=algo-1, epoch=207, train loss <loss>=1.05649433136\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:40 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:41 INFO 139712694728512] Epoch[208] Batch[0] avg_epoch_loss=0.911082\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:41 INFO 139712694728512] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=0.911081910133\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:43 INFO 139712694728512] Epoch[208] Batch[5] avg_epoch_loss=1.003631\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:43 INFO 139712694728512] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=1.00363091628\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:43 INFO 139712694728512] Epoch[208] Batch [5]#011Speed: 180.79 samples/sec#011loss=1.003631\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] Epoch[208] Batch[10] avg_epoch_loss=0.985198\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=208, batch=10 train loss <loss>=0.963079190254\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] Epoch[208] Batch [10]#011Speed: 180.14 samples/sec#011loss=0.963079\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4411.577939987183, \"sum\": 4411.577939987183, \"min\": 4411.577939987183}}, \"EndTime\": 1597164105.310675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164100.898618}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=146.882392065 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] #quality_metric: host=algo-1, epoch=208, train loss <loss>=0.98519831354\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:45 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:46 INFO 139712694728512] Epoch[209] Batch[0] avg_epoch_loss=0.979575\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:46 INFO 139712694728512] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=0.979574501514\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:47 INFO 139712694728512] Epoch[209] Batch[5] avg_epoch_loss=0.989294\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:47 INFO 139712694728512] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=0.989294171333\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:47 INFO 139712694728512] Epoch[209] Batch [5]#011Speed: 176.17 samples/sec#011loss=0.989294\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:49 INFO 139712694728512] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4045.2308654785156, \"sum\": 4045.2308654785156, \"min\": 4045.2308654785156}}, \"EndTime\": 1597164109.356527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164105.31075}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:49 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=151.531369001 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:49 INFO 139712694728512] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:49 INFO 139712694728512] #quality_metric: host=algo-1, epoch=209, train loss <loss>=0.976792788506\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:49 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:50 INFO 139712694728512] Epoch[210] Batch[0] avg_epoch_loss=0.710557\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:50 INFO 139712694728512] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=0.710557222366\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:51 INFO 139712694728512] Epoch[210] Batch[5] avg_epoch_loss=0.949544\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:51 INFO 139712694728512] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=0.949543913205\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:51 INFO 139712694728512] Epoch[210] Batch [5]#011Speed: 181.08 samples/sec#011loss=0.949544\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:53 INFO 139712694728512] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4050.175905227661, \"sum\": 4050.175905227661, \"min\": 4050.175905227661}}, \"EndTime\": 1597164113.407291, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164109.356623}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:53 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=152.333942068 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:53 INFO 139712694728512] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:53 INFO 139712694728512] #quality_metric: host=algo-1, epoch=210, train loss <loss>=0.957381415367\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:53 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:54 INFO 139712694728512] Epoch[211] Batch[0] avg_epoch_loss=0.995079\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:54 INFO 139712694728512] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=0.995078980923\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:55 INFO 139712694728512] Epoch[211] Batch[5] avg_epoch_loss=0.907669\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:55 INFO 139712694728512] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=0.907669117053\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:55 INFO 139712694728512] Epoch[211] Batch [5]#011Speed: 184.07 samples/sec#011loss=0.907669\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] Epoch[211] Batch[10] avg_epoch_loss=0.944719\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=211, batch=10 train loss <loss>=0.989179229736\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] Epoch[211] Batch [10]#011Speed: 181.85 samples/sec#011loss=0.989179\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4326.855897903442, \"sum\": 4326.855897903442, \"min\": 4326.855897903442}}, \"EndTime\": 1597164117.734762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164113.407387}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=160.620078394 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] #quality_metric: host=algo-1, epoch=211, train loss <loss>=0.944719168273\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:57 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:58 INFO 139712694728512] Epoch[212] Batch[0] avg_epoch_loss=0.899345\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:41:58 INFO 139712694728512] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=0.899344563484\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:00 INFO 139712694728512] Epoch[212] Batch[5] avg_epoch_loss=0.935928\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:00 INFO 139712694728512] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=0.935927708944\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:00 INFO 139712694728512] Epoch[212] Batch [5]#011Speed: 182.26 samples/sec#011loss=0.935928\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:01 INFO 139712694728512] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4011.795997619629, \"sum\": 4011.795997619629, \"min\": 4011.795997619629}}, \"EndTime\": 1597164121.747089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164117.734849}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:01 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=154.289711811 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:01 INFO 139712694728512] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:01 INFO 139712694728512] #quality_metric: host=algo-1, epoch=212, train loss <loss>=0.908469772339\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:01 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:02 INFO 139712694728512] Epoch[213] Batch[0] avg_epoch_loss=0.752335\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:02 INFO 139712694728512] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=0.752335190773\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:04 INFO 139712694728512] Epoch[213] Batch[5] avg_epoch_loss=0.989036\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:04 INFO 139712694728512] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=0.98903598388\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:04 INFO 139712694728512] Epoch[213] Batch [5]#011Speed: 182.43 samples/sec#011loss=0.989036\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] Epoch[213] Batch[10] avg_epoch_loss=0.943559\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=213, batch=10 train loss <loss>=0.888986289501\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] Epoch[213] Batch [10]#011Speed: 182.60 samples/sec#011loss=0.888986\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4319.164037704468, \"sum\": 4319.164037704468, \"min\": 4319.164037704468}}, \"EndTime\": 1597164126.06689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164121.747187}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.127843285 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=213, train loss <loss>=0.943558850072\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] Epoch[214] Batch[0] avg_epoch_loss=0.988285\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:06 INFO 139712694728512] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=0.988285183907\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:08 INFO 139712694728512] Epoch[214] Batch[5] avg_epoch_loss=1.048264\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:08 INFO 139712694728512] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=1.04826374849\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:08 INFO 139712694728512] Epoch[214] Batch [5]#011Speed: 180.56 samples/sec#011loss=1.048264\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:10 INFO 139712694728512] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4049.7710704803467, \"sum\": 4049.7710704803467, \"min\": 4049.7710704803467}}, \"EndTime\": 1597164130.117193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164126.066977}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:10 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=158.028479517 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:10 INFO 139712694728512] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=214, train loss <loss>=1.02592196465\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:10 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:10 INFO 139712694728512] Epoch[215] Batch[0] avg_epoch_loss=1.102163\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:10 INFO 139712694728512] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=1.10216283798\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:12 INFO 139712694728512] Epoch[215] Batch[5] avg_epoch_loss=0.989595\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:12 INFO 139712694728512] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=0.989594598611\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:12 INFO 139712694728512] Epoch[215] Batch [5]#011Speed: 178.49 samples/sec#011loss=0.989595\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:14 INFO 139712694728512] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4056.7450523376465, \"sum\": 4056.7450523376465, \"min\": 4056.7450523376465}}, \"EndTime\": 1597164134.174496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164130.117286}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:14 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.143435765 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:14 INFO 139712694728512] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:14 INFO 139712694728512] #quality_metric: host=algo-1, epoch=215, train loss <loss>=1.02223330736\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:14 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:15 INFO 139712694728512] Epoch[216] Batch[0] avg_epoch_loss=1.035300\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:15 INFO 139712694728512] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=1.03530013561\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:16 INFO 139712694728512] Epoch[216] Batch[5] avg_epoch_loss=1.017639\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:16 INFO 139712694728512] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=1.01763854424\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:16 INFO 139712694728512] Epoch[216] Batch [5]#011Speed: 179.10 samples/sec#011loss=1.017639\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] Epoch[216] Batch[10] avg_epoch_loss=1.034782\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=216, batch=10 train loss <loss>=1.05535482168\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] Epoch[216] Batch [10]#011Speed: 182.37 samples/sec#011loss=1.055355\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4396.438837051392, \"sum\": 4396.438837051392, \"min\": 4396.438837051392}}, \"EndTime\": 1597164138.571503, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164134.17459}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=148.751646351 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] #quality_metric: host=algo-1, epoch=216, train loss <loss>=1.03478230671\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:18 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:19 INFO 139712694728512] Epoch[217] Batch[0] avg_epoch_loss=0.953215\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:19 INFO 139712694728512] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=0.953214883804\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:21 INFO 139712694728512] Epoch[217] Batch[5] avg_epoch_loss=1.023610\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:21 INFO 139712694728512] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=1.02361040314\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:21 INFO 139712694728512] Epoch[217] Batch [5]#011Speed: 181.33 samples/sec#011loss=1.023610\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] Epoch[217] Batch[10] avg_epoch_loss=0.954743\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=217, batch=10 train loss <loss>=0.872101122141\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] Epoch[217] Batch [10]#011Speed: 179.17 samples/sec#011loss=0.872101\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4400.8469581604, \"sum\": 4400.8469581604, \"min\": 4400.8469581604}}, \"EndTime\": 1597164142.972893, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164138.57159}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] #throughput_metric: host=algo-1, train throughput=147.694649346 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] #quality_metric: host=algo-1, epoch=217, train loss <loss>=0.954742548141\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] loss did not improve\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:22 INFO 139712694728512] Loading parameters from best epoch (177)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 68.1760311126709, \"sum\": 68.1760311126709, \"min\": 68.1760311126709}}, \"EndTime\": 1597164143.041654, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164142.97298}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 INFO 139712694728512] stopping training now\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 INFO 139712694728512] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 INFO 139712694728512] Final loss: 0.902268461206 (occurred at epoch 177)\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 INFO 139712694728512] #quality_metric: host=algo-1, train final_loss <loss>=0.902268461206\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 WARNING 139712694728512] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 INFO 139712694728512] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 WARNING 139712694728512] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:23 INFO 139712694728512] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 3042.6111221313477, \"sum\": 3042.6111221313477, \"min\": 3042.6111221313477}}, \"EndTime\": 1597164146.085244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164143.041738}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:26 INFO 139712694728512] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 3616.5809631347656, \"sum\": 3616.5809631347656, \"min\": 3616.5809631347656}}, \"EndTime\": 1597164146.659169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164146.085379}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:26 INFO 139712694728512] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:26 INFO 139712694728512] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 85.24894714355469, \"sum\": 85.24894714355469, \"min\": 85.24894714355469}}, \"EndTime\": 1597164146.744561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164146.65926}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:26 INFO 139712694728512] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[08/11/2020 16:42:26 INFO 139712694728512] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 935380.4359436035, \"sum\": 935380.4359436035, \"min\": 935380.4359436035}, \"setuptime\": {\"count\": 1, \"max\": 9.593963623046875, \"sum\": 9.593963623046875, \"min\": 9.593963623046875}}, \"EndTime\": 1597164146.879011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1597164146.744623}\n",
      "\u001b[0m\n",
      "\n",
      "2020-08-11 16:42:43 Uploading - Uploading generated training model\n",
      "2020-08-11 16:42:43 Completed - Training job completed\n",
      "Training seconds: 994\n",
      "Billable seconds: 994\n",
      "CPU times: user 2.55 s, sys: 119 ms, total: 2.67 s\n",
      "Wall time: 19min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": train_s3\n",
    "}\n",
    "\n",
    "estimator.fit(data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + datetime.timedelta(minutes=10)\n",
    "#         prediction_time = 144\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "#         prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)       \n",
    "#         print(prediction_time)\n",
    "#         print(type(prediction_time))\n",
    "#         print(prediction_length)\n",
    "#         print(type(prediction_length))\n",
    "#         print(freq)\n",
    "#         print(type(freq))\n",
    "        \n",
    "        prediction_index = pd.date_range(prediction_time, prediction_time + freq * (prediction_length-1), freq=freq)\n",
    "#         print(prediction_index)\n",
    "        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6668ed30685f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m predictor = estimator.deploy(\n\u001b[1;32m      3\u001b[0m     \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredictor_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor, \n",
    "    wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infs = pd.Series(data[0]['target'][:-144])\n",
    "infs.index=pd.date_range(data[0]['start'], datetime.datetime.strptime(data[0]['start'],  '%Y-%m-%d %H:%M:%S') + datetime.timedelta(minutes=10*2015), freq='10T')\n",
    "infs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(ts= infs, \n",
    "                               dynamic_feat=data[0]['dynamic_feat'],\n",
    "                               quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infsf = pd.Series(data[0]['target'])\n",
    "infsf.index=pd.date_range(data[0]['start'], datetime.datetime.strptime(data[0]['start'],  '%Y-%m-%d %H:%M:%S') + datetime.timedelta(minutes=10*2159), freq='10T')\n",
    "infsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(infsf['2012-03-13 23:20:00':])\n",
    "plt.plot(prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
